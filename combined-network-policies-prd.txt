---
id: NP-00
title: Create Kubernetes Cluster with Network Policy Support
type: setup
category: infrastructure
priority: high
status: ready
estimated_time: 10m
dependencies: []
tags: [gke, kubernetes, network-policy, cluster-setup]
---

## Overview

Create a Kubernetes cluster with Network Policies feature enabled to support all subsequent network policy recipes and configurations.

## Objectives

- Provision a Kubernetes cluster with Network Policy support
- Configure Calico as the networking provider
- Validate cluster is ready for network policy deployments

## Background

Most Kubernetes installation methods do not provide a cluster with Network Policies feature enabled by default. Manual installation and configuration of a Network Policy provider (such as Weave Net or Calico) is typically required.

**Google Kubernetes Engine (GKE)** provides an easy path to get a Kubernetes cluster with Network Policies feature pre-configured. GKE automatically configures Calico as the networking provider (generally available as of GKE v1.10).

## Requirements

### Task 1: Create GKE Cluster with Network Policy
**Priority:** High
**Status:** pending

Create a GKE cluster named `np` with Network Policy feature enabled.

**Actions:**
- Run gcloud command to create cluster
- Enable network policy flag
- Specify zone for cluster deployment
- Create 3-node cluster configuration

**Command:**
```bash
gcloud container clusters create np \
    --enable-network-policy \
    --zone us-central1-b
```

## Acceptance Criteria

- [ ] GKE cluster created successfully with name `np`
- [ ] Network Policy feature enabled
- [ ] Calico networking provider configured
- [ ] 3-node cluster operational
- [ ] Cluster accessible via kubectl

## Technical Specifications

**Cluster Configuration:**
- Name: `np`
- Provider: Google Kubernetes Engine (GKE)
- Network Policy Provider: Calico
- Node Count: 3
- Zone: us-central1-b
- Kubernetes Version: v1.10+

## Verification

Verify cluster creation:
```bash
kubectl get nodes
kubectl get pods --all-namespaces
```

## Cleanup

### Task: Delete Cluster
When tutorial is complete, remove the cluster:

```bash
gcloud container clusters delete -q --zone us-central1-b np
```

## References

- [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine/)
- [Kubernetes Network Policies Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)

## Notes

This cluster serves as the foundation for all network policy recipes in this repository. Ensure this cluster is created before attempting any subsequent tutorials.
---
id: NP-01
title: Deny All Traffic to an Application
type: policy
category: basics
priority: high
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, deny-all, isolation, security, ingress]
---

## Overview

Implement a NetworkPolicy that drops all traffic to pods of an application using Pod Selectors to achieve complete network isolation.

## Objectives

- Create a deny-all NetworkPolicy for specific application pods
- Block all incoming traffic to selected pods
- Verify traffic isolation is working correctly
- Understand the foundation for whitelisting traffic

## Background

This NetworkPolicy drops all traffic to pods of an application selected using Pod Selectors. This is a fundamental pattern in Kubernetes network security.

**Use Cases:**
- Start whitelisting traffic using Network Policies (first blacklist, then whitelist)
- Run a Pod and prevent any other Pods from communicating with it
- Temporarily isolate traffic to a Service from other Pods
- Implement zero-trust networking principles

![Diagram for DENY all traffic to an application policy](img/1.gif)

## Requirements

### Task 1: Deploy Test Application
**Priority:** High
**Status:** pending

Deploy a test nginx application to demonstrate the policy.

**Actions:**
- Run nginx Pod with label `app=web`
- Expose the Pod at port 80
- Verify initial connectivity

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Test Initial Connectivity
**Priority:** High
**Status:** pending

Verify that the web service is accessible before applying the policy.

**Actions:**
- Run temporary test Pod
- Make HTTP request to web Service
- Confirm successful response

**Command:**
```bash
kubectl run --rm -i -t --image=alpine test-$RANDOM -- sh
# Inside the pod:
wget -qO- http://web
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
...
```

### Task 3: Create and Apply NetworkPolicy
**Priority:** High
**Status:** pending

Create and apply the deny-all NetworkPolicy manifest.

**Actions:**
- Create `web-deny-all.yaml` manifest
- Apply policy to cluster
- Verify policy creation

**Manifest:** `web-deny-all.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-deny-all
spec:
  podSelector:
    matchLabels:
      app: web
  ingress: []
```

**Command:**
```bash
kubectl apply -f web-deny-all.yaml
```

**Expected Output:**
```
networkpolicy "web-deny-all" created
```

### Task 4: Verify Traffic is Blocked
**Priority:** High
**Status:** pending

Test that traffic is now blocked by the NetworkPolicy.

**Actions:**
- Run new temporary test Pod
- Attempt HTTP request with timeout
- Confirm request times out

**Command:**
```bash
kubectl run --rm -i -t --image=alpine test-$RANDOM -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web
```

**Expected Result:**
```
wget: download timed out
```

## Acceptance Criteria

- [ ] Nginx pod deployed with label `app=web`
- [ ] Service exposed on port 80
- [ ] Initial connectivity verified (before policy)
- [ ] NetworkPolicy `web-deny-all` created successfully
- [ ] Policy targets pods with `app=web` label
- [ ] All traffic to web pod is blocked
- [ ] Connection timeout occurs when accessing web service

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-deny-all`
- API Version: `networking.k8s.io/v1`
- Pod Selector: `app=web`
- Ingress Rules: None (empty array)

**How It Works:**
- Targets Pods with `app=web` label
- Missing `spec.ingress` field means no traffic is allowed
- Empty ingress array `[]` blocks all incoming traffic
- Pods without matching labels are unaffected

## Implementation Details

The manifest targets Pods with `app=web` label to police the network. The `spec.ingress` field is empty, therefore not allowing any traffic into the Pod.

**Important Notes:**
- If you create another NetworkPolicy that gives some Pods access to this application (directly or indirectly), this NetworkPolicy will be obsolete
- If there is at least one NetworkPolicy with a rule allowing traffic, it means traffic will be routed to the pod regardless of policies blocking it
- NetworkPolicies are additive - their union is evaluated

## Verification

Verify the policy exists:
```bash
kubectl get networkpolicy
kubectl describe networkpolicy web-deny-all
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod web
kubectl delete service web
kubectl delete networkpolicy web-deny-all
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [NetworkPolicy API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#networkpolicy-v1-networking-k8s-io)

## Notes

This is a fundamental policy that serves as the starting point for implementing whitelisting strategies. Always test policies in a non-production environment first.
---
id: NP-02A
title: Allow All Traffic to an Application
type: policy
category: basics
priority: medium
status: ready
estimated_time: 10m
dependencies: [NP-00, NP-01]
tags: [network-policy, allow-all, ingress, override, permissive]
---

## Overview

Create a NetworkPolicy that explicitly allows all traffic to an application, overriding any restrictive policies and enabling access from all pods in all namespaces.

## Objectives

- Override deny-all policies with an explicit allow-all rule
- Enable unrestricted access to an application from all namespaces
- Understand how NetworkPolicy rules combine additively
- Demonstrate policy precedence and override behavior

## Background

After applying a [deny-all](01-deny-all-traffic-to-an-application.md) policy that blocks all non-whitelisted traffic to an application, this policy allows access from all pods in the current namespace and other namespaces.

**Important:** Applying this policy makes any other policies restricting traffic to the pod void, allowing all traffic from any namespace. NetworkPolicies are additive - if at least one policy allows traffic, it will flow regardless of other blocking policies.

**Use Cases:**
- Temporarily open access to a service for debugging
- Override restrictive policies for shared services
- Create exceptions for common infrastructure components
- Explicitly document that a service should be accessible from anywhere

## Requirements

### Task 1: Deploy Test Application
**Priority:** High
**Status:** pending

Start a web application to demonstrate the policy.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create and Apply Allow-All Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy manifest that allows all traffic.

**Actions:**
- Create `web-allow-all.yaml` manifest
- Configure empty ingress rule to allow all traffic
- Apply policy to cluster

**Manifest:** `web-allow-all.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-all
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - {}
```

**Key Points:**
- `namespace: default` deploys this policy to the `default` namespace
- `podSelector` applies the ingress rule to pods with `app: web` label
- Empty ingress rule `{}` allows traffic from all pods in all namespaces
- Equivalent to explicitly specifying:
  ```yaml
  - from:
    - podSelector: {}
      namespaceSelector: {}
  ```

**Command:**
```bash
kubectl apply -f web-allow-all.yaml
```

**Expected Output:**
```
networkpolicy "web-allow-all" created
```

### Task 3: Test with Deny-All Policy (Optional)
**Priority:** Medium
**Status:** pending

Optionally apply the [`web-deny-all` policy](01-deny-all-traffic-to-an-application.md) to validate that `web-allow-all` overrides it.

**Actions:**
- Apply web-deny-all policy
- Verify web-allow-all takes precedence
- Traffic should still be allowed

**Command:**
```bash
kubectl apply -f web-deny-all.yaml
```

### Task 4: Verify Traffic is Allowed
**Priority:** High
**Status:** pending

Test that traffic flows freely to the application.

**Actions:**
- Run temporary test pod
- Make HTTP request to web service
- Confirm successful response

**Command:**
```bash
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web
```

**Expected Result:**
```html
<!DOCTYPE html>
<html><head>
...
```

Traffic is allowed!

## Acceptance Criteria

- [ ] Web pod deployed with label `app=web`
- [ ] Service exposed on port 80
- [ ] NetworkPolicy `web-allow-all` created successfully
- [ ] Policy targets pods with `app=web` label
- [ ] Empty ingress rule configured correctly
- [ ] Traffic from any pod in any namespace is allowed
- [ ] Policy overrides any existing deny policies
- [ ] HTTP requests succeed from test pods

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-allow-all`
- API Version: `networking.k8s.io/v1`
- Namespace: `default`
- Pod Selector: `app=web`
- Ingress Rules: Empty rule `{}`

**How It Works:**
- Empty ingress rule `{}` matches all sources
- Allows traffic from all pods in current namespace
- Allows traffic from all pods in all other namespaces
- NetworkPolicies are additive - allow rules take precedence
- If any policy allows traffic, it flows regardless of deny policies

**Policy Precedence:**
- NetworkPolicies combine additively (union of all rules)
- An allow rule in any policy enables the traffic
- Cannot create explicit "deny" rules
- Default behavior without policies: allow all
- Default behavior with policies: deny all except allowed

## Implementation Details

The empty ingress rule is the key to this policy:

```yaml
ingress:
- {}
```

This is equivalent to:
```yaml
ingress:
- from:
  - podSelector: {}
    namespaceSelector: {}
```

Both forms select all pods from all namespaces, but the empty form `{}` is more concise.

## Verification

Check policy status:
```bash
kubectl get networkpolicy
kubectl describe networkpolicy web-allow-all
```

Test from different namespaces:
```bash
kubectl create namespace test-ns
kubectl run test-$RANDOM --namespace=test-ns --rm -i -t --image=alpine -- sh
# wget -qO- http://web.default
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod,service web
kubectl delete networkpolicy web-allow-all web-deny-all
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [NetworkPolicy Deny-All Pattern](01-deny-all-traffic-to-an-application.md)
- [Network Policy Best Practices](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies)

## Notes

Use this policy carefully in production environments. While it can be useful for shared services or debugging, it effectively disables network segmentation for the target application. Consider whether a more restrictive policy with explicit namespace or pod selectors would be more appropriate.

This pattern demonstrates the additive nature of NetworkPolicies - once any policy allows traffic, that traffic will flow regardless of other policies.
---
id: NP-02
title: Limit Traffic to an Application
type: policy
category: basics
priority: high
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, whitelist, pod-selector, ingress, microservices]
---

## Overview

Create a NetworkPolicy that allows traffic from only specific Pods using label selectors, implementing a whitelist-based security model.

## Objectives

- Restrict traffic to a service only from authorized microservices
- Implement pod-to-pod communication policies using labels
- Demonstrate whitelist-based network security
- Verify both blocked and allowed traffic patterns

## Background

NetworkPolicies allow you to create rules that permit traffic from only certain Pods based on label selectors. This is essential for implementing zero-trust networking and microservices security.

**Use Cases:**
- Restrict traffic to a service only to other microservices that need to use it
- Restrict connections to a database only to the application using it
- Implement service mesh security without additional infrastructure
- Control inter-service communication in microservices architecture

![Diagram of LIMIT traffic to an application policy](img/2.gif)

## Requirements

### Task 1: Deploy API Server Application
**Priority:** High
**Status:** pending

Deploy a REST API server application with specific labels.

**Actions:**
- Run nginx as API server with labels `app=bookstore` and `role=api`
- Expose the service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run apiserver --image=nginx --labels="app=bookstore,role=api" --expose --port=80
```

### Task 2: Create NetworkPolicy
**Priority:** High
**Status:** pending

Create NetworkPolicy to restrict access only to pods with matching labels.

**Actions:**
- Create `api-allow.yaml` manifest
- Configure pod selector for target pods
- Configure ingress rules for allowed sources
- Apply policy to cluster

**Manifest:** `api-allow.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: api-allow
spec:
  podSelector:
    matchLabels:
      app: bookstore
      role: api
  ingress:
  - from:
      - podSelector:
          matchLabels:
            app: bookstore
```

**Command:**
```bash
kubectl apply -f api-allow.yaml
```

**Expected Output:**
```
networkpolicy "api-allow" created
```

### Task 3: Test Blocked Traffic
**Priority:** High
**Status:** pending

Verify that pods without the required label cannot access the API server.

**Actions:**
- Run test pod without `app=bookstore` label
- Attempt HTTP connection with timeout
- Confirm connection is blocked

**Command:**
```bash
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://apiserver
```

**Expected Result:**
```
wget: download timed out
```

Traffic is blocked!

### Task 4: Test Allowed Traffic
**Priority:** High
**Status:** pending

Verify that pods with the correct label can access the API server.

**Actions:**
- Run test pod with `app=bookstore` label
- Make HTTP connection
- Confirm successful response

**Command:**
```bash
kubectl run test-$RANDOM --rm -i -t --image=alpine --labels="app=bookstore,role=frontend" -- sh
# Inside the pod:
wget -qO- --timeout=2 http://apiserver
```

**Expected Result:**
```html
<!DOCTYPE html>
<html><head>
```

Traffic is allowed!

## Acceptance Criteria

- [ ] API server pod deployed with labels `app=bookstore` and `role=api`
- [ ] Service exposed on port 80
- [ ] NetworkPolicy `api-allow` created successfully
- [ ] Policy targets pods with correct labels
- [ ] Traffic from non-matching pods is blocked (timeout)
- [ ] Traffic from matching pods is allowed (successful response)
- [ ] Policy only affects specified pods, not cluster-wide

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `api-allow`
- API Version: `networking.k8s.io/v1`
- Target Pods: `app=bookstore` AND `role=api`
- Allowed Sources: Pods with `app=bookstore` label

**How It Works:**
- `podSelector` specifies which pods the policy applies to (target pods)
- `ingress.from.podSelector` specifies which pods can send traffic (source pods)
- Only pods matching the source selector can connect
- All other traffic is denied by default
- Policy is namespace-scoped

**Label Matching:**
- Target: Pods must have BOTH `app=bookstore` AND `role=api` labels
- Source: Pods need only `app=bookstore` label to access
- Labels on source pods determine access rights
- Non-matching pods are automatically blocked

## Implementation Details

The NetworkPolicy uses label selectors to create fine-grained access control:

1. **Target Selection**: The `spec.podSelector` matches pods with both `app=bookstore` and `role=api` labels
2. **Source Selection**: The `ingress.from.podSelector` allows traffic from any pod with `app=bookstore` label
3. **Default Deny**: Once a NetworkPolicy targets a pod, all non-matching traffic is denied
4. **Namespace Scope**: Both selectors operate within the same namespace by default

## Verification

Check policy status:
```bash
kubectl get networkpolicy
kubectl describe networkpolicy api-allow
kubectl get pods --show-labels
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod apiserver
kubectl delete service apiserver
kubectl delete networkpolicy api-allow
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Network Policy Recipes](https://github.com/ahmetb/kubernetes-network-policy-recipes)
- [Pod Selector Specification](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

## Notes

This pattern is fundamental for microservices security. Use consistent labeling strategies across your applications to make NetworkPolicy management easier. Consider documenting your label taxonomy for the team.
---
id: NP-03
title: Deny All Non-Whitelisted Traffic in a Namespace
type: policy
category: namespaces
priority: critical
status: ready
estimated_time: 10m
dependencies: [NP-00]
tags: [network-policy, deny-all, namespace, default-deny, best-practice, zero-trust]
---

## Overview

Implement a fundamental default-deny NetworkPolicy that blocks all cross-pod networking in a namespace except traffic explicitly whitelisted via other Network Policies.

## Objectives

- Establish default-deny posture for namespace security
- Block all pod-to-pod traffic by default
- Create foundation for whitelist-based network policies
- Implement zero-trust networking principles at namespace level

## Background

This is a fundamental policy that blocks all cross-pod networking except connections explicitly whitelisted via other Network Policies deployed in the namespace.

**Use Case:** This is a foundational security practice for any namespace where workloads are deployed (except system namespaces like `kube-system`).

**Best Practice:** This policy provides default "deny all" functionality, allowing you to clearly identify component dependencies and deploy Network Policies that translate to dependency graphs between components. Start with this policy, then explicitly whitelist necessary traffic.

![Diagram of DENY all non-whitelisted traffic policy](img/3.gif)

## Requirements

### Task 1: Create Default Deny-All Policy
**Priority:** Critical
**Status:** pending

Create and apply the default deny-all NetworkPolicy manifest.

**Actions:**
- Create `default-deny-all.yaml` manifest
- Configure empty pod selector (matches all pods)
- Configure empty ingress rules (denies all traffic)
- Apply to target namespace

**Manifest:** `default-deny-all.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny-all
  namespace: default
spec:
  podSelector: {}
  ingress: []
```

**Key Configuration Points:**
- `namespace: default` - Deploy to the `default` namespace (change as needed)
- `podSelector: {}` - Empty selector matches ALL pods in the namespace
- `ingress: []` - Empty ingress array denies all incoming traffic

**How It Works:**
- Empty `podSelector` means policy applies to all pods in namespace
- Empty `ingress` array means no traffic is allowed
- Can also omit `ingress` field entirely (same effect)
- Only affects pods in the specified namespace

**Command:**
```bash
kubectl apply -f default-deny-all.yaml
```

**Expected Output:**
```
networkpolicy "default-deny-all" created
```

### Task 2: Verify Policy Deployment
**Priority:** High
**Status:** pending

Confirm the policy was created and is active.

**Actions:**
- Check policy exists
- Verify policy configuration
- Review affected pods

**Commands:**
```bash
kubectl get networkpolicy -n default
kubectl describe networkpolicy default-deny-all -n default
kubectl get pods -n default
```

## Acceptance Criteria

- [ ] NetworkPolicy `default-deny-all` created in target namespace
- [ ] Policy applies to all pods (empty podSelector)
- [ ] All ingress traffic is blocked (empty ingress rules)
- [ ] Policy is active and enforced
- [ ] Existing pods in namespace are now protected
- [ ] New pods deployed to namespace automatically protected
- [ ] Traffic between pods in namespace is blocked by default

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `default-deny-all`
- API Version: `networking.k8s.io/v1`
- Scope: Namespace-level (applies to all pods in specified namespace)
- Pod Selector: `{}` (empty - matches all pods)
- Ingress Rules: `[]` (empty array - denies all ingress)

**Behavior Details:**
- Applies to all pods in the namespace immediately
- Blocks all incoming traffic to all pods
- Does not affect egress (outbound) traffic
- Does not affect pods in other namespaces
- Other NetworkPolicies can whitelist specific traffic

**Namespace Considerations:**
- Deploy to application namespaces
- Do NOT deploy to `kube-system` namespace (will break cluster)
- Consider deploying to all non-system namespaces as default
- Can be overridden by additional NetworkPolicies

## Implementation Details

This policy is the cornerstone of a zero-trust network architecture in Kubernetes:

1. **Default Deny**: Without this policy, all traffic is allowed by default
2. **Explicit Allow**: After applying this, you must explicitly whitelist necessary traffic
3. **Additive Policies**: Additional NetworkPolicies can allow specific traffic
4. **Dependency Mapping**: Forces you to document and understand service dependencies

**Alternative Syntax:**

These three forms are equivalent:
```yaml
# Form 1: Empty array (recommended)
ingress: []

# Form 2: Null value
ingress:

# Form 3: Omitted field
spec:
  podSelector: {}
  # ingress field omitted
```

## Verification

Verify the policy is working by attempting to connect between pods:

```bash
# Deploy two test pods
kubectl run pod1 --image=nginx -n default
kubectl run pod2 --image=alpine -n default -- sleep 3600

# Try to connect from pod2 to pod1 (should fail)
kubectl exec pod2 -n default -- wget -qO- --timeout=2 http://pod1
# Expected: timeout or connection refused
```

## Cleanup

### Task: Remove Policy
To remove the default-deny policy:

```bash
kubectl delete networkpolicy default-deny-all -n default
```

**Warning:** Only remove this policy if you understand the security implications. Removing it will allow all traffic between pods in the namespace.

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Network Policy Best Practices](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies)
- [Zero Trust Networking](https://www.nist.gov/publications/zero-trust-architecture)

## Notes

**Best Practices:**
- Apply this policy to every application namespace
- Apply before deploying applications (easier to whitelist from the start)
- Document all whitelisted traffic flows
- Review and audit NetworkPolicies regularly
- Test policies in non-production first

**Common Pitfalls:**
- Do NOT apply to `kube-system` - will break cluster functionality
- Remember to whitelist health checks and readiness probes
- DNS traffic may need to be whitelisted explicitly
- Service mesh sidecars may need special consideration

This policy is the foundation of network security in Kubernetes. Consider it a requirement for production deployments.
---
id: NP-04
title: Deny All Traffic from Other Namespaces
type: policy
category: namespaces
priority: high
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, namespace-isolation, multi-tenancy, security]
---

## Overview

Configure a NetworkPolicy to deny all traffic from other namespaces while allowing all traffic within the same namespace, implementing namespace-level isolation.

## Objectives

- Block all cross-namespace traffic
- Allow intra-namespace communication
- Implement namespace isolation for multi-tenancy
- Prevent accidental cross-environment communication

## Background

This policy denies all traffic from other namespaces while allowing all traffic coming from the same namespace where the pod is deployed. This is also known as "LIMIT access to the current namespace".

**Use Cases:**
- Prevent deployments in `test` namespace from accidentally sending traffic to services or databases in `prod` namespace
- Host applications from different customers in separate Kubernetes namespaces and block traffic coming from outside a namespace
- Implement multi-tenant cluster architecture
- Create environment isolation (dev, staging, prod)

![Diagram of DENY all traffic from other namespaces policy](img/4.gif)

## Requirements

### Task 1: Deploy Test Application
**Priority:** High
**Status:** pending

Start a web service in the default namespace.

**Actions:**
- Deploy nginx pod with label `app=web` in default namespace
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --namespace=default --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create Namespace Isolation Policy
**Priority:** High
**Status:** pending

Create and apply NetworkPolicy that allows only same-namespace traffic.

**Actions:**
- Create `deny-from-other-namespaces.yaml` manifest
- Configure empty pod selector (applies to all pods)
- Configure ingress rule for same-namespace only
- Apply to target namespace

**Manifest:** `deny-from-other-namespaces.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: default
  name: deny-from-other-namespaces
spec:
  podSelector:
    matchLabels:
  ingress:
  - from:
    - podSelector: {}
```

**Key Configuration Points:**
- `namespace: default` - Deploys it to the `default` namespace
- `spec.podSelector.matchLabels` - Empty selector applies policy to ALL pods in namespace
- `spec.ingress.from.podSelector: {}` - Empty selector allows traffic from ALL pods in the same namespace only
- No `namespaceSelector` specified - restricts traffic to current namespace only

**Command:**
```bash
kubectl apply -f deny-from-other-namespaces.yaml
```

**Expected Output:**
```
networkpolicy "deny-from-other-namespaces" created
```

### Task 3: Test Cross-Namespace Blocking
**Priority:** High
**Status:** pending

Verify that traffic from other namespaces is blocked.

**Actions:**
- Create test namespace `foo`
- Run test pod in `foo` namespace
- Attempt connection to web service in default namespace
- Confirm connection is blocked

**Commands:**
```bash
kubectl create namespace foo
kubectl run test-$RANDOM --namespace=foo --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```
wget: download timed out
```

Traffic from `foo` namespace is blocked!

### Task 4: Test Same-Namespace Access
**Priority:** High
**Status:** pending

Verify that traffic within the same namespace is allowed.

**Actions:**
- Run test pod in default namespace
- Attempt connection to web service
- Confirm connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=default --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
```

Traffic within default namespace works fine!

## Acceptance Criteria

- [ ] Web service deployed in default namespace
- [ ] NetworkPolicy `deny-from-other-namespaces` created
- [ ] Policy applies to all pods in namespace (empty podSelector)
- [ ] Traffic from other namespaces is blocked (timeout)
- [ ] Traffic within same namespace is allowed (successful response)
- [ ] Test namespace created for verification
- [ ] Cross-namespace traffic blocked consistently

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `deny-from-other-namespaces`
- API Version: `networking.k8s.io/v1`
- Scope: All pods in specified namespace
- Pod Selector: Empty (matches all pods)
- Ingress Source: All pods in same namespace only

**How It Works:**
- Empty `podSelector` applies policy to all pods in the namespace
- `ingress.from.podSelector: {}` allows traffic from all pods
- Absence of `namespaceSelector` restricts source to current namespace
- Traffic from other namespaces is implicitly denied

**Selector Behavior:**
- `podSelector: {}` alone = same namespace only
- `podSelector: {}` + `namespaceSelector: {}` = all namespaces
- No selector = deny all

## Implementation Details

This policy leverages the default namespace-scoping behavior of podSelector:

**Key Insight:** When you specify a `podSelector` without a `namespaceSelector` in an ingress rule, it only matches pods in the same namespace as the NetworkPolicy.

**Equivalent Explicit Form:**
```yaml
ingress:
- from:
  - podSelector: {}
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
```

However, omitting `namespaceSelector` is simpler and more maintainable.

## Verification

Check policy status:
```bash
kubectl get networkpolicy -n default
kubectl describe networkpolicy deny-from-other-namespaces -n default
```

List namespaces:
```bash
kubectl get namespaces
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod web -n default
kubectl delete service web -n default
kubectl delete networkpolicy deny-from-other-namespaces -n default
kubectl delete namespace foo
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Namespace Isolation Patterns](https://kubernetes.io/docs/concepts/services-networking/network-policies/#targeting-a-namespace-by-its-name)
- [Multi-tenancy Best Practices](https://kubernetes.io/docs/concepts/security/multi-tenancy/)

## Notes

**Best Practices:**
- Apply this policy to all production namespaces
- Use namespace labels to organize environments
- Consider combining with PodSecurityPolicies for defense in depth
- Document namespace communication requirements

**Common Patterns:**
- Environment Isolation: Separate dev, staging, prod namespaces
- Team Isolation: Separate namespaces per team or project
- Customer Isolation: Separate namespaces per customer (multi-tenancy)

**Important Considerations:**
- This policy allows ALL pods within the namespace to communicate
- Consider combining with pod-level policies for finer control
- Remember to allow traffic from ingress controllers if needed
- System namespaces (kube-system) may need special handling

This policy is essential for multi-tenant Kubernetes clusters and environment isolation strategies.
---
id: NP-05
title: Allow Traffic from All Namespaces
type: policy
category: namespaces
priority: medium
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, cross-namespace, shared-services, allow]
---

## Overview

Create a NetworkPolicy that allows traffic from all pods in all namespaces to a particular application, enabling cross-namespace access to shared services.

## Objectives

- Enable cross-namespace access to specific applications
- Allow traffic from all namespaces to shared services
- Override restrictive namespace policies for specific workloads
- Support common services accessible cluster-wide

## Background

This NetworkPolicy allows traffic from all pods in all namespaces to reach a particular application. This pattern is useful for shared infrastructure services that need to be accessible from multiple namespaces.

**Use Cases:**
- Common service or database used by deployments in different namespaces
- Shared monitoring or logging endpoints
- Central authentication services
- Internal API gateways serving multiple teams

**Important:** You do not need this policy unless there is already a NetworkPolicy [blocking traffic to the application](01-deny-all-traffic-to-an-application.md) or a NetworkPolicy [blocking non-whitelisted traffic to all pods in the namespace](03-deny-all-non-whitelisted-traffic-in-the-namespace.md).

![Diagram of ALLOW traffic to an application from all namespaces policy](img/5.gif)

## Requirements

### Task 1: Deploy Shared Service
**Priority:** High
**Status:** pending

Start a web service on default namespace.

**Actions:**
- Deploy nginx pod with label `app=web` in default namespace
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --namespace=default --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create Allow-All-Namespaces Policy
**Priority:** High
**Status:** pending

Create and apply NetworkPolicy that allows traffic from all namespaces.

**Actions:**
- Create `web-allow-all-namespaces.yaml` manifest
- Configure pod selector for target application
- Configure ingress rule with empty namespaceSelector
- Apply to cluster

**Manifest:** `web-allow-all-namespaces.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  namespace: default
  name: web-allow-all-namespaces
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - from:
    - namespaceSelector: {}
```

**Key Configuration Points:**
- Applies policy only to `app:web` pods in `default` namespace
- `namespaceSelector: {}` selects all pods in all namespaces
- By default, omitting `namespaceSelector` would only allow traffic from the same namespace

**Alternative Syntax:**

Dropping all selectors from `spec.ingress.from` has the same effect:
```yaml
ingress:
  - from:
```

However, the explicit `namespaceSelector: {}` form is preferred for clarity.

**Command:**
```bash
kubectl apply -f web-allow-all-namespaces.yaml
```

**Expected Output:**
```
networkpolicy "web-allow-all-namespaces" created
```

### Task 3: Create Test Namespace
**Priority:** High
**Status:** pending

Create a secondary namespace for testing cross-namespace access.

**Actions:**
- Create namespace called `secondary`
- Verify namespace exists

**Command:**
```bash
kubectl create namespace secondary
```

### Task 4: Test Cross-Namespace Access
**Priority:** High
**Status:** pending

Verify that traffic from other namespaces is allowed.

**Actions:**
- Run test pod in secondary namespace
- Attempt connection to web service in default namespace
- Confirm connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
```

Traffic from secondary namespace works!

## Acceptance Criteria

- [ ] Web service deployed in default namespace with label `app=web`
- [ ] Service exposed on port 80
- [ ] NetworkPolicy `web-allow-all-namespaces` created
- [ ] Policy targets only pods with `app=web` label
- [ ] Empty namespaceSelector configured
- [ ] Test namespace `secondary` created
- [ ] Traffic from secondary namespace is allowed
- [ ] Traffic from default namespace also works
- [ ] Cross-namespace communication verified

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-allow-all-namespaces`
- API Version: `networking.k8s.io/v1`
- Namespace: `default`
- Pod Selector: `app=web`
- Ingress Source: All pods in all namespaces

**How It Works:**
- `podSelector` targets specific application pods
- `namespaceSelector: {}` allows traffic from all namespaces
- Empty namespace selector matches all namespaces
- Pods matching the source criteria can connect

**Selector Combinations:**
```yaml
# Allow from all pods in all namespaces
namespaceSelector: {}

# Allow from all pods in same namespace only (default)
# (no namespaceSelector specified)

# Allow from specific namespace
namespaceSelector:
  matchLabels:
    name: production

# Allow from specific pods in all namespaces
namespaceSelector: {}
podSelector:
  matchLabels:
    role: frontend
```

## Implementation Details

The `namespaceSelector: {}` is the critical component:

**Without namespaceSelector:**
```yaml
ingress:
- from:
  - podSelector: {}
# Allows only from same namespace
```

**With empty namespaceSelector:**
```yaml
ingress:
- from:
  - namespaceSelector: {}
# Allows from all namespaces
```

**Combining both selectors:**
```yaml
ingress:
- from:
  - podSelector: {}
    namespaceSelector: {}
# Allows from all pods in all namespaces (same as namespaceSelector alone)
```

## Verification

Check policy status:
```bash
kubectl get networkpolicy -n default
kubectl describe networkpolicy web-allow-all-namespaces -n default
```

Test from multiple namespaces:
```bash
# Test from default namespace
kubectl run test-$RANDOM --namespace=default --rm -i -t --image=alpine -- sh

# Test from another namespace
kubectl run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod web -n default
kubectl delete service web -n default
kubectl delete networkpolicy web-allow-all-namespaces -n default
kubectl delete namespace secondary
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Namespace Selector Documentation](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- [Deny All Traffic Pattern](01-deny-all-traffic-to-an-application.md)
- [Namespace Default Deny Pattern](03-deny-all-non-whitelisted-traffic-in-the-namespace.md)

## Notes

**Best Practices:**
- Use this pattern sparingly - prefer restrictive policies
- Document why cross-namespace access is needed
- Consider using service mesh for more sophisticated routing
- Monitor cross-namespace traffic for unexpected patterns

**Security Considerations:**
- This policy essentially makes the application "public" within the cluster
- All namespaces include user workloads and system namespaces
- Consider whether specific namespace selection would be more appropriate
- Combine with authentication/authorization at application level

**Common Use Cases:**
- Central logging aggregators (e.g., Elasticsearch)
- Shared databases (with proper authentication)
- Monitoring endpoints (e.g., Prometheus)
- Service mesh control planes
- DNS servers
- Internal API gateways

This policy is useful for shared infrastructure but should be applied judiciously with proper security controls at the application layer.
---
id: NP-06
title: Allow Traffic from a Specific Namespace
type: policy
category: namespaces
priority: high
status: ready
estimated_time: 20m
dependencies: [NP-00]
tags: [network-policy, namespace-selector, cross-namespace, label-based]
---

## Overview

Create a NetworkPolicy that allows traffic from all pods in a specific namespace selected by labels, enabling selective cross-namespace communication.

## Objectives

- Allow traffic from specific namespaces using label selectors
- Implement label-based namespace access control
- Restrict database access to production namespaces only
- Enable monitoring tools to scrape metrics across namespaces

## Background

This policy is similar to [allowing traffic from all namespaces](05-allow-traffic-from-all-namespaces.md) but demonstrates how to choose particular namespaces using label selectors.

**Use Cases:**
- Restrict traffic to a production database only to namespaces where production workloads are deployed
- Enable monitoring tools deployed to a particular namespace to scrape metrics from the current namespace
- Allow specific teams' namespaces to access shared services
- Implement environment-based access control (prod-to-prod, dev-to-dev)

![Diagram of ALLOW all traffic from a namespace policy](img/6.gif)

## Requirements

### Task 1: Deploy Web Server
**Priority:** High
**Status:** pending

Run a web server in the `default` namespace.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create and Label Namespaces
**Priority:** High
**Status:** pending

Create test namespaces with appropriate labels.

**Actions:**
- Create `dev` namespace with label `purpose=testing`
- Create `prod` namespace with label `purpose=production`
- Verify namespace labels

**Namespace Setup:**
- `default`: Where the API is deployed (installed by Kubernetes)
- `prod`: Production workloads run here (label: `purpose=production`)
- `dev`: Dev/test area (label: `purpose=testing`)

**Commands:**
```bash
# Create dev namespace
kubectl create namespace dev
kubectl label namespace/dev purpose=testing

# Create prod namespace
kubectl create namespace prod
kubectl label namespace/prod purpose=production
```

**Verify labels:**
```bash
kubectl get namespaces --show-labels
```

### Task 3: Create Namespace-Selective Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that restricts traffic to pods from production namespace only.

**Actions:**
- Create `web-allow-prod.yaml` manifest
- Configure namespace selector with label matching
- Apply policy to cluster

**Manifest:** `web-allow-prod.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-prod
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production
```

**Key Configuration:**
- Targets pods with `app: web` label
- Allows traffic only from namespaces with `purpose=production` label
- Uses `matchLabels` to select specific namespaces

**Command:**
```bash
kubectl apply -f web-allow-prod.yaml
```

**Expected Output:**
```
networkpolicy "web-allow-prod" created
```

### Task 4: Test Traffic from Dev Namespace (Blocked)
**Priority:** High
**Status:** pending

Verify that traffic from dev namespace is blocked.

**Actions:**
- Run test pod in dev namespace
- Attempt connection to web service in default namespace
- Confirm connection is blocked

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```
wget: download timed out
```

Traffic is blocked from dev namespace!

### Task 5: Test Traffic from Prod Namespace (Allowed)
**Priority:** High
**Status:** pending

Verify that traffic from prod namespace is allowed.

**Actions:**
- Run test pod in prod namespace
- Attempt connection to web service in default namespace
- Confirm connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
...
```

Traffic is allowed from prod namespace!

## Acceptance Criteria

- [ ] Web service deployed in default namespace
- [ ] Dev namespace created with label `purpose=testing`
- [ ] Prod namespace created with label `purpose=production`
- [ ] NetworkPolicy `web-allow-prod` created successfully
- [ ] Policy uses namespaceSelector with matchLabels
- [ ] Traffic from dev namespace is blocked (timeout)
- [ ] Traffic from prod namespace is allowed (successful response)
- [ ] Only production-labeled namespaces can access the service

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-allow-prod`
- Pod Selector: `app=web`
- Ingress Source: Namespaces with `purpose=production` label

**How It Works:**
- `podSelector` targets the application pods
- `namespaceSelector.matchLabels` selects source namespaces
- Only pods in matching namespaces can send traffic
- Namespace labels are used for access control
- Multiple namespaces can match if they have the same label

**Label-Based Selection:**
```yaml
# Select namespaces by single label
namespaceSelector:
  matchLabels:
    purpose: production

# Select namespaces by multiple labels (AND)
namespaceSelector:
  matchLabels:
    purpose: production
    team: platform

# Select using matchExpressions (more flexible)
namespaceSelector:
  matchExpressions:
  - key: environment
    operator: In
    values: [production, staging]
```

## Implementation Details

This policy uses namespace labels for access control, which is a powerful pattern for organizing Kubernetes multi-tenancy:

**Best Practices for Namespace Labels:**
- `purpose`: testing, production, development
- `team`: platform, frontend, backend, data
- `environment`: dev, staging, prod
- `criticality`: low, medium, high, critical

**Label Strategy:**
- Use consistent label taxonomy across the organization
- Document label meanings and usage
- Automate label assignment where possible
- Regularly audit namespace labels

## Verification

Check policy and namespace configuration:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy web-allow-prod

# View namespace labels
kubectl get namespaces --show-labels

# View specific namespace
kubectl describe namespace prod
kubectl describe namespace dev
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete networkpolicy web-allow-prod
kubectl delete pod web
kubectl delete service web
kubectl delete namespace prod dev
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Namespace Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors)
- [Multi-tenancy Best Practices](https://kubernetes.io/docs/concepts/security/multi-tenancy/)

## Notes

**Best Practices:**
- Establish a namespace labeling convention
- Document label meanings in your organization
- Use labels to represent environments, teams, or criticality levels
- Combine with RBAC for comprehensive access control
- Regularly audit which namespaces can access critical services

**Common Patterns:**
```yaml
# Environment-based access
purpose: production  # Only prod namespaces

# Team-based access
team: data-science  # Only data science team

# Criticality-based access
tier: critical  # Only critical infrastructure

# Multiple criteria (all must match)
matchLabels:
  environment: production
  region: us-east
```

This pattern is essential for implementing fine-grained multi-tenancy and environment isolation in Kubernetes clusters.
---
id: NP-07
title: Allow Traffic from Some Pods in Another Namespace
type: policy
category: namespaces
priority: high
status: ready
estimated_time: 25m
dependencies: [NP-00]
tags: [network-policy, namespace-selector, pod-selector, intersection, kubernetes-v1.11]
---

## Overview

Implement a NetworkPolicy that uses the AND operation to combine podSelector and namespaceSelector, allowing traffic only from specific pods in specific namespaces.

## Objectives

- Use combined podSelector and namespaceSelector with AND operation
- Allow traffic only from specific pods in labeled namespaces
- Understand the difference between AND and OR conditions in NetworkPolicy
- Restrict access to monitoring pods in operations namespaces

## Background

Since Kubernetes v1.11, it is possible to combine `podSelector` and `namespaceSelector` with an AND (intersection) operation. This enables fine-grained control over which pods in which namespaces can access your services.

**Use Cases:**
- Allow only monitoring pods from operations namespaces to scrape metrics
- Restrict database access to specific application pods in production namespaces
- Enable debugging tools in specific namespaces to access target services
- Implement fine-grained multi-tenant access control

**Important Notes:**
- This feature is available on Kubernetes v1.11 or after
- Most networking plugins do not yet support this feature
- Make sure to test this policy after deployment to verify it works correctly

## Requirements

### Task 1: Deploy Web Server
**Priority:** High
**Status:** pending

Run a web application in the default namespace.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create and Label Target Namespace
**Priority:** High
**Status:** pending

Create a namespace with appropriate labels for testing.

**Actions:**
- Create `other` namespace
- Label namespace with `team=operations`
- Verify namespace label

**Commands:**
```bash
kubectl create namespace other
kubectl label namespace/other team=operations
```

**Verification:**
```bash
kubectl get namespace other --show-labels
```

### Task 3: Create Combined Selector Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that uses AND operation to restrict traffic to specific pods in specific namespaces.

**Actions:**
- Create `web-allow-all-ns-monitoring.yaml` manifest
- Configure combined namespace and pod selectors
- Apply policy to cluster

**Manifest:** `web-allow-all-ns-monitoring.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-all-ns-monitoring
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
    - from:
      - namespaceSelector:     # chooses all pods in namespaces labelled with team=operations
          matchLabels:
            team: operations
        podSelector:           # chooses pods with type=monitoring
          matchLabels:
            type: monitoring
```

**Key Configuration:**
- Targets pods with `app: web` label
- Uses AND condition: namespace must have `team=operations` AND pod must have `type=monitoring`
- Both conditions must be true for traffic to be allowed

**Command:**
```bash
kubectl apply -f web-allow-all-ns-monitoring.yaml
```

**Expected Output:**
```
networkpolicy.networking.k8s.io/web-allow-all-ns-monitoring created
```

### Task 4: Test from Default Namespace Without Labels (Blocked)
**Priority:** High
**Status:** pending

Verify traffic from default namespace without proper labels is blocked.

**Actions:**
- Run test pod in default namespace without labels
- Attempt connection to web service
- Confirm connection is blocked

**Commands:**
```bash
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```
wget: download timed out
```

Traffic is blocked (namespace is not labeled `team=operations`)!

### Task 5: Test from Default Namespace With Monitoring Label (Blocked)
**Priority:** High
**Status:** pending

Verify that even with pod label, traffic is blocked from wrong namespace.

**Actions:**
- Run test pod with `type=monitoring` label in default namespace
- Attempt connection to web service
- Confirm connection is still blocked

**Commands:**
```bash
kubectl run test-$RANDOM --labels="type=monitoring" --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```
wget: download timed out
```

Traffic is blocked (default namespace doesn't have `team=operations` label)!

### Task 6: Test from Other Namespace Without Labels (Blocked)
**Priority:** High
**Status:** pending

Verify traffic from other namespace without pod label is blocked.

**Actions:**
- Run test pod in other namespace without pod labels
- Attempt connection to web service
- Confirm connection is blocked

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=other --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```
wget: download timed out
```

Traffic is blocked (pod doesn't have `type=monitoring` label)!

### Task 7: Test from Other Namespace With Monitoring Label (Allowed)
**Priority:** High
**Status:** pending

Verify traffic is allowed when both conditions are met.

**Actions:**
- Run test pod with `type=monitoring` label in other namespace (labeled `team=operations`)
- Attempt connection to web service
- Confirm connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --namespace=other --labels="type=monitoring" --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://web.default
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
...
```

Traffic is allowed (both conditions are met)!

## Acceptance Criteria

- [ ] Web service deployed in default namespace
- [ ] Other namespace created with label `team=operations`
- [ ] NetworkPolicy `web-allow-all-ns-monitoring` created successfully
- [ ] Policy uses combined podSelector and namespaceSelector (AND operation)
- [ ] Traffic from default namespace is blocked (wrong namespace)
- [ ] Traffic from default namespace with monitoring label is blocked (wrong namespace)
- [ ] Traffic from other namespace without monitoring label is blocked (wrong pod)
- [ ] Traffic from other namespace with monitoring label is allowed (both conditions met)

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-allow-all-ns-monitoring`
- Namespace: `default`
- Pod Selector: `app=web`
- Ingress Source: Pods with `type=monitoring` in namespaces with `team=operations`

**How It Works:**
- The policy uses AND (intersection) operation
- Namespace must have label `team=operations`
- AND pod must have label `type=monitoring`
- Both conditions are required - missing either one blocks traffic
- The selectors are at the same indentation level (same list item in `from`)

**AND vs OR Conditions:**

**OR Condition (separate list items):**
```yaml
ingress:
  - from:
    - namespaceSelector:     # OR condition
        matchLabels:
          team: operations
    - podSelector:           # These are separate items
        matchLabels:
          type: monitoring
```
This allows traffic from:
- ANY pod in namespaces with `team=operations` OR
- ANY pod with `type=monitoring` in the same namespace

**AND Condition (same list item):**
```yaml
ingress:
  - from:
    - namespaceSelector:     # AND condition
        matchLabels:
          team: operations
      podSelector:           # These are combined
        matchLabels:
          type: monitoring
```
This allows traffic from:
- Pods with `type=monitoring` in namespaces with `team=operations`
- Both conditions MUST be true

## Implementation Details

**Understanding the Intersection:**
- When `namespaceSelector` and `podSelector` are in the same `from` entry, they are ANDed together
- This creates an intersection: pods must match BOTH selectors
- The namespace is selected first, then pods within that namespace are filtered
- This enables precise access control across namespace boundaries

**YAML Structure Matters:**
```yaml
# Same list item (AND)
- from:
  - namespaceSelector: {...}
    podSelector: {...}

# Different list items (OR)
- from:
  - namespaceSelector: {...}
  - podSelector: {...}
```

**Real-World Examples:**
```yaml
# Allow only monitoring pods from ops namespaces
- from:
  - namespaceSelector:
      matchLabels:
        team: operations
    podSelector:
      matchLabels:
        role: monitoring

# Allow debug pods from troubleshooting namespace
- from:
  - namespaceSelector:
      matchLabels:
        purpose: troubleshooting
    podSelector:
      matchLabels:
        tool: debugger

# Allow specific app pods from production namespaces
- from:
  - namespaceSelector:
      matchLabels:
        environment: production
    podSelector:
      matchLabels:
        app: frontend
```

## Verification

Check policy configuration:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy web-allow-all-ns-monitoring

# View namespace labels
kubectl get namespaces --show-labels

# Check specific namespace
kubectl describe namespace other
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete networkpolicy web-allow-all-ns-monitoring
kubectl delete namespace other
kubectl delete pod web
kubectl delete service web
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [NetworkPolicy v1.11 Release Notes](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/)
- [Label Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

## Notes

**Important Considerations:**
- This feature requires Kubernetes v1.11+
- Not all CNI plugins support this feature - verify with your network provider
- Always test policies after deployment to ensure they work as expected
- The AND operation provides fine-grained control but requires careful label management

**Best Practices:**
- Document your namespace labeling scheme
- Use consistent label naming across namespaces
- Test both positive and negative cases (allowed and blocked traffic)
- Consider using label namespaces (e.g., `team.company.com/name`)
- Monitor policy effectiveness with network policy logs

**Common Mistakes:**
- Confusing AND vs OR syntax (indentation matters!)
- Forgetting to label namespaces
- Not testing with all combinations of labels
- Assuming the feature is supported by all CNI plugins

**Debugging Tips:**
```bash
# Check if namespace has correct labels
kubectl get namespace <name> --show-labels

# Check if pod has correct labels
kubectl get pods --show-labels -n <namespace>

# Describe NetworkPolicy to see what it's selecting
kubectl describe networkpolicy <name>

# Check CNI plugin version and features
kubectl get pods -n kube-system
```

This advanced pattern is essential for implementing sophisticated multi-tenant access control in Kubernetes clusters where namespace and pod-level isolation is required.
---
id: NP-08
title: Allow Traffic from External Clients
type: policy
category: basics
priority: high
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, external-traffic, load-balancer, ingress, allow-all]
---

## Overview

Create a NetworkPolicy that enables external clients from the public Internet (directly or via a Load Balancer) to access pods in a namespace that otherwise denies all non-whitelisted traffic.

## Objectives

- Enable external access to pods in a restricted namespace
- Configure NetworkPolicy to allow all sources
- Expose services via Load Balancer
- Understand how to whitelist external traffic

## Background

This NetworkPolicy enables external clients from the public Internet directly or via a Load Balancer to access pods. This is essential for exposing services to end users while maintaining network security policies.

**Use Cases:**
- Expose web applications to the public Internet in a namespace denying all non-whitelisted traffic
- Enable external access to API gateways in secured namespaces
- Allow Load Balancers to reach backend services
- Implement public-facing services while maintaining internal network restrictions

![Diagram of ALLOW traffic from external clients policy](img/8.gif)

## Requirements

### Task 1: Deploy and Expose Web Server
**Priority:** High
**Status:** pending

Run a web server and expose it to the internet with a Load Balancer.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose pod with LoadBalancer service type
- Wait for external IP assignment

**Commands:**
```bash
kubectl run web --image=nginx --labels="app=web" --port=80

kubectl expose pod/web --type=LoadBalancer
```

**Verification:**
Wait until an EXTERNAL-IP appears on `kubectl get service` output:

```bash
kubectl get service web -w
```

**Expected Output:**
```
NAME   TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE
web    LoadBalancer   10.0.0.123     35.123.45.67    80:32000/TCP   2m
```

### Task 2: Verify Initial External Access
**Priority:** High
**Status:** pending

Visit the external IP and confirm the service is accessible.

**Actions:**
- Retrieve external IP from service
- Access service via web browser or curl
- Confirm nginx welcome page loads

**Command:**
```bash
# Get the external IP
EXTERNAL_IP=$(kubectl get service web -o jsonpath='{.status.loadBalancer.ingress[0].ip}')

# Test access
curl http://$EXTERNAL_IP
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

### Task 3: Create External Access Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that allows traffic from all sources (internal and external).

**Actions:**
- Create `web-allow-external.yaml` manifest
- Configure to allow all ingress traffic
- Apply policy to cluster

**Manifest:** `web-allow-external.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-external
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - {}
```

**Key Configuration:**
- Targets pods with `app: web` label
- Single ingress rule with empty selector
- No specific podSelector or namespaceSelector means allow from all sources
- Allows both cluster-internal and external traffic

**Command:**
```bash
kubectl apply -f web-allow-external.yaml
```

**Expected Output:**
```
networkpolicy "web-allow-external" created
```

### Task 4: Verify External Access Still Works
**Priority:** High
**Status:** pending

Confirm that external access continues to work after policy application.

**Actions:**
- Access external IP via browser or curl
- Verify nginx welcome page still loads
- Confirm no connection errors

**Command:**
```bash
curl http://$EXTERNAL_IP
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
```

External access continues to work!

## Acceptance Criteria

- [ ] Nginx pod deployed with label `app=web`
- [ ] Service exposed as LoadBalancer type
- [ ] External IP assigned to service
- [ ] Service accessible from external clients before policy
- [ ] NetworkPolicy `web-allow-external` created successfully
- [ ] Policy allows traffic from all sources
- [ ] External access continues to work after policy application
- [ ] Internal cluster traffic also allowed

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `web-allow-external`
- Pod Selector: `app=web`
- Ingress Rules: Single empty rule allowing all sources

**How It Works:**
- The manifest specifies one ingress rule for `app=web` pods
- Since it does not specify a particular `podSelector` or `namespaceSelector`, it allows traffic from all resources
- Empty ingress rule `{}` means "allow from anywhere"
- This includes both external traffic and internal cluster traffic
- The policy whitelists all traffic sources

**Traffic Flow:**
```
External Client
       ↓
Load Balancer (External IP)
       ↓
Kubernetes Service (ClusterIP)
       ↓
NetworkPolicy (allows all sources)
       ↓
Pod (app=web)
```

**Important Notes:**
- This policy allows ALL traffic, both external and internal
- It's useful in namespaces with default-deny policies
- The Load Balancer handles external routing
- NetworkPolicy operates at the pod level, not service level

## Implementation Details

**Understanding Empty Ingress Rules:**

```yaml
# Allow from all sources (internal and external)
ingress:
- {}

# Equivalent to (but more concise than):
ingress:
- from: []  # Empty from array means "from anywhere"
```

**Port Restriction Example:**
To restrict external access only to port 80, you can deploy an ingress rule such as:

```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-external-port-80
spec:
  podSelector:
    matchLabels:
      app: web
  ingress:
  - ports:
    - port: 80
      protocol: TCP
```

This allows traffic to port 80 from any source but blocks other ports.

**Multiple Port Example:**
```yaml
ingress:
- ports:
  - port: 80
    protocol: TCP
  - port: 443
    protocol: TCP
```

**Combining with Source Restrictions:**
```yaml
# Allow external traffic only to specific ports
# while blocking other traffic
ingress:
- ports:
  - port: 80
  - port: 443
# Note: no 'from' section means from anywhere
```

## Verification

Check policy and service configuration:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy web-allow-external

# View service details
kubectl get service web
kubectl describe service web

# Test internal access (from another pod)
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- http://web

# Test external access
curl http://$EXTERNAL_IP
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod web
kubectl delete service web
kubectl delete networkpolicy web-allow-external
```

**Note:** If using a cloud provider, ensure the Load Balancer is fully deleted to avoid charges:

```bash
# Verify Load Balancer is removed
kubectl get service
# Should not show the web service
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Kubernetes Services - LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer)
- [Ingress vs LoadBalancer](https://kubernetes.io/docs/concepts/services-networking/ingress/)

## Notes

**Best Practices:**
- In production, prefer Ingress controllers over LoadBalancer for HTTP/HTTPS traffic
- Always use TLS/SSL for external traffic (port 443)
- Combine with other security measures (authentication, rate limiting)
- Consider using specific source IP ranges if possible (more restrictive)
- Monitor external access with logging and metrics

**Security Considerations:**
- This policy allows ALL traffic from any source
- Does not provide authentication or authorization
- Consider additional security layers:
  - WAF (Web Application Firewall)
  - DDoS protection
  - Rate limiting
  - Authentication at application level
  - TLS termination

**Alternative Approaches:**
```yaml
# Restrict to specific external IP ranges (CIDR blocks)
ingress:
- from:
  - ipBlock:
      cidr: 203.0.113.0/24
  ports:
  - port: 80

# Allow from Load Balancer source ranges
ingress:
- from:
  - ipBlock:
      cidr: 0.0.0.0/0
      except:
      - 169.254.169.254/32  # Block metadata service
  ports:
  - port: 80
```

**Cloud Provider Considerations:**
- **GKE:** Load Balancer automatically created
- **EKS:** Requires AWS Load Balancer Controller
- **AKS:** Azure Load Balancer automatically provisioned
- **On-premises:** May require MetalLB or similar

**Testing External Access:**
```bash
# Using curl
curl -v http://$EXTERNAL_IP

# Using wget
wget -O- http://$EXTERNAL_IP

# Check headers
curl -I http://$EXTERNAL_IP

# Test from different locations
curl http://$EXTERNAL_IP --resolve web.example.com:80:$EXTERNAL_IP
```

**Common Issues:**
- External IP pending: Wait for cloud provider provisioning
- Connection refused: Check pod is running and healthy
- Timeout: Verify firewall rules allow traffic to Load Balancer
- Policy not applied: Check podSelector matches pod labels

This pattern is essential for exposing public-facing services in Kubernetes while maintaining network security policies for internal traffic.
---
id: NP-09
title: Allow Traffic Only to a Port of an Application
type: policy
category: advanced
priority: high
status: ready
estimated_time: 20m
dependencies: [NP-00]
tags: [network-policy, port-restriction, named-ports, metrics, monitoring]
---

## Overview

Create a NetworkPolicy that restricts ingress traffic to specific ports of an application, enabling fine-grained access control at the port level.

## Objectives

- Define ingress rules for specific ports
- Use both numerical and named ports in policies
- Allow monitoring access to metrics endpoints while restricting application access
- Understand port-based network segmentation

## Background

This NetworkPolicy lets you define ingress rules for specific ports of an application. If you do not specify a port in the ingress rules, the rule applies to all ports. A port may be either a numerical or named port on a pod.

**Use Cases:**
- Allow monitoring system to collect metrics by querying the diagnostics port without giving access to the application
- Enable health check probes to specific endpoints
- Restrict database access to specific port ranges
- Separate admin interfaces from user-facing interfaces

![Diagram of ALLOW traffic only to a port of an application policy](img/9.gif)

## Requirements

### Task 1: Deploy Multi-Port Application
**Priority:** High
**Status:** pending

Run a web server application that listens on multiple ports.

**Actions:**
- Deploy application pod with label `app=apiserver`
- Application responds on port 8000 for main traffic
- Application responds on port 5000 for metrics

**Command:**
```bash
kubectl run apiserver --image=ahmet/app-on-two-ports --labels="app=apiserver"
```

**Application Details:**
- Returns hello response on `http://:8000/`
- Returns monitoring metrics on `http://:5000/metrics`

### Task 2: Expose Application as Service
**Priority:** High
**Status:** pending

Create a Service to expose both application ports.

**Actions:**
- Create ClusterIP service for apiserver
- Map port 8001 to container port 8000
- Map port 5001 to container port 5000

**Command:**
```bash
kubectl create service clusterip apiserver \
    --tcp 8001:8000 \
    --tcp 5001:5000
```

**Important Note:**
Network Policies will not know the port numbers you exposed in the Service (8001 and 5001). This is because they control inter-pod traffic and when you expose a Pod as Service, ports are remapped. Therefore, you need to use the Pod port numbers (8000 and 5000) in the NetworkPolicy specification.

An alternative less error-prone approach is to refer to port names (such as `metrics` and `http`).

### Task 3: Create Port-Specific Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that allows traffic only to the metrics port from monitoring pods.

**Actions:**
- Create `api-allow-5000.yaml` manifest
- Configure port restriction for port 5000
- Allow traffic only from pods with `role=monitoring`
- Apply policy to cluster

**Manifest:** `api-allow-5000.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: api-allow-5000
spec:
  podSelector:
    matchLabels:
      app: apiserver
  ingress:
  - ports:
    - port: 5000
    from:
    - podSelector:
        matchLabels:
          role: monitoring
```

**Key Configuration:**
- Targets pods with `app: apiserver` label
- Allows traffic only to port 5000
- Only from pods with `role: monitoring` label
- Drops all other traffic (including port 8000)

**Command:**
```bash
kubectl apply -f api-allow-5000.yaml
```

**Expected Output:**
```
networkpolicy "api-allow-5000" created
```

**Policy Effects:**
- Drops all non-whitelisted traffic to `app=apiserver`
- Allows traffic on port 5000 from pods with label `role=monitoring` in the same namespace
- Blocks traffic to port 8000 from all sources

### Task 4: Test Access Without Labels (Blocked)
**Priority:** High
**Status:** pending

Verify that traffic from pods without proper labels is blocked.

**Actions:**
- Run test pod without custom labels
- Attempt connection to both ports
- Confirm both connections are blocked

**Commands:**
```bash
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://apiserver:8001
wget -qO- --timeout=2 http://apiserver:5001/metrics
```

**Expected Result:**
```
wget: download timed out
wget: download timed out
```

Both ports are blocked (pod doesn't have `role=monitoring` label)!

### Task 5: Test Access With Monitoring Label
**Priority:** High
**Status:** pending

Verify that monitoring pods can access port 5000 but not port 8000.

**Actions:**
- Run test pod with `role=monitoring` label
- Attempt connection to port 8000 (blocked)
- Attempt connection to port 5000 (allowed)
- Verify metrics response

**Commands:**
```bash
kubectl run test-$RANDOM --labels="role=monitoring" --rm -i -t --image=alpine -- sh
# Inside the pod:
wget -qO- --timeout=2 http://apiserver:8001
```

**Expected Result:**
```
wget: download timed out
```

Port 8000 is blocked (not in allowed ports)!

```bash
# Inside the same pod:
wget -qO- --timeout=2 http://apiserver:5001/metrics
```

**Expected Result:**
```
http.requests=3
go.goroutines=5
go.cpus=1
```

Port 5000 is accessible!

## Acceptance Criteria

- [ ] Multi-port application deployed with label `app=apiserver`
- [ ] Service exposes both ports (8001 and 5001)
- [ ] NetworkPolicy `api-allow-5000` created successfully
- [ ] Policy specifies port 5000 restriction
- [ ] Policy requires `role=monitoring` label
- [ ] Traffic to both ports blocked without proper labels
- [ ] Traffic to port 8000 blocked even with monitoring label
- [ ] Traffic to port 5000 allowed from monitoring pods
- [ ] Metrics endpoint accessible only to monitoring pods

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `api-allow-5000`
- Pod Selector: `app=apiserver`
- Allowed Port: 5000
- Source Selector: `role=monitoring`

**How It Works:**
- NetworkPolicy operates at pod level, not service level
- Port numbers in policy refer to container ports, not service ports
- When port is specified, only that port is whitelisted
- All other ports are implicitly denied
- The `from` selector further restricts which pods can access the port

**Port Specification:**
```yaml
# Numerical port
ports:
- port: 5000
  protocol: TCP

# Named port (preferred)
ports:
- port: metrics
  protocol: TCP
```

**Service vs Container Ports:**
```
Service Port 5001 → Container Port 5000
                    ↑
            NetworkPolicy checks this port
```

## Implementation Details

**Using Named Ports (Recommended):**

Named ports are less error-prone and more maintainable:

```yaml
# NetworkPolicy with named port
ingress:
- ports:
  - port: api-port
```

**Corresponding Pod Spec:**
```yaml
containers:
- name: api
  image: api-image:latest
  ports:
  - name: api-port
    containerPort: 5000
    protocol: TCP
```

**Benefits of Named Ports:**
- More readable and self-documenting
- Easier to change port numbers without updating policies
- Reduces configuration errors
- Better for multi-container pods

**Multiple Port Example:**
```yaml
spec:
  podSelector:
    matchLabels:
      app: myapp
  ingress:
  - ports:
    - port: 8080  # HTTP
    - port: 9090  # Metrics
    from:
    - podSelector:
        matchLabels:
          role: monitoring
  - ports:
    - port: 8080  # HTTP only
    from:
    - podSelector:
        matchLabels:
          role: frontend
```

**Different Rules for Different Ports:**
```yaml
spec:
  podSelector:
    matchLabels:
      app: database
  ingress:
  # Admin access to all ports
  - from:
    - podSelector:
        matchLabels:
          role: admin
  # App access only to port 5432
  - ports:
    - port: 5432
    from:
    - podSelector:
        matchLabels:
          role: application
```

**Protocol Specification:**
```yaml
ports:
- port: 5000
  protocol: TCP  # TCP (default), UDP, or SCTP

# Multiple protocols for same port
- port: 53
  protocol: UDP
- port: 53
  protocol: TCP
```

## Verification

Check policy and test connectivity:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy api-allow-5000

# View service and endpoints
kubectl get service apiserver
kubectl get endpoints apiserver

# Check pod ports
kubectl get pod -l app=apiserver -o jsonpath='{.items[0].spec.containers[0].ports}'

# Test from monitoring pod
kubectl run monitor --labels="role=monitoring" --rm -i -t --image=alpine -- sh
# Inside:
nc -zv apiserver 5000  # Should work
nc -zv apiserver 8000  # Should timeout

# Test from regular pod
kubectl run regular --rm -i -t --image=alpine -- sh
# Inside:
nc -zv apiserver 5000  # Should timeout
nc -zv apiserver 8000  # Should timeout
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod apiserver
kubectl delete service apiserver
kubectl delete networkpolicy api-allow-5000
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Container Ports](https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/)
- [Named Ports](https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services)

## Notes

**Best Practices:**
- Use named ports whenever possible for better maintainability
- Document which ports serve which purposes
- Separate administrative interfaces from user-facing ports
- Use different policies for different port access patterns
- Test all port combinations (allowed and blocked)

**Common Use Cases:**
```yaml
# Web application with separate admin interface
ingress:
# Public access to web port
- ports:
  - port: 80
    name: http
  from: []  # Allow from anywhere

# Admin access to admin port
- ports:
  - port: 8080
    name: admin
  from:
  - podSelector:
      matchLabels:
        role: admin
```

**Monitoring Pattern:**
```yaml
# Application with Prometheus metrics
ingress:
# App traffic from frontend only
- ports:
  - port: 8000
  from:
  - podSelector:
      matchLabels:
        tier: frontend

# Metrics from Prometheus only
- ports:
  - port: 9090
    name: metrics
  from:
  - namespaceSelector:
      matchLabels:
        name: monitoring
    podSelector:
      matchLabels:
        app: prometheus
```

**Database Pattern:**
```yaml
# PostgreSQL with separate replication port
ingress:
# Application queries
- ports:
  - port: 5432
  from:
  - podSelector:
      matchLabels:
        app: backend

# Replication traffic
- ports:
  - port: 5433
  from:
  - podSelector:
      matchLabels:
        role: postgres-replica
```

**Debugging Tips:**
```bash
# Check if container port is listening
kubectl exec -it <pod> -- netstat -tlnp

# Check if service maps to correct ports
kubectl describe service <name>

# View NetworkPolicy in detail
kubectl get networkpolicy <name> -o yaml

# Test specific port connectivity
kubectl run test --rm -i -t --image=nicolaka/netshoot -- bash
# Inside:
nc -zv <service> <port>
telnet <service> <port>
```

**Common Mistakes:**
- Using service ports instead of container ports in policy
- Forgetting to specify protocol (defaults to TCP)
- Not testing all combinations of ports and labels
- Confusing named ports with service port names
- Assuming port restrictions apply to service level

**Security Considerations:**
- Port-based segmentation is a defense-in-depth strategy
- Should be combined with application-level authentication
- Consider using mTLS for sensitive ports
- Monitor access to restricted ports
- Regularly audit port access policies

This pattern is essential for implementing the principle of least privilege at the network level, ensuring components can only access the specific ports they need.
---
id: NP-10
title: Allow Traffic from Apps Using Multiple Selectors
type: policy
category: advanced
priority: high
status: ready
estimated_time: 20m
dependencies: [NP-00]
tags: [network-policy, multiple-selectors, or-condition, microservices, shared-services]
---

## Overview

Create a NetworkPolicy that uses multiple pod selectors to allow traffic from several different applications, enabling shared services accessible to multiple microservices.

## Objectives

- Define multiple pod selectors in a single NetworkPolicy
- Understand OR logic in ingress rules
- Enable shared databases or services for multiple microservices
- Implement fine-grained access control with multiple sources

## Background

NetworkPolicy lets you define multiple pod selectors to allow traffic from different sources. This is particularly useful for shared services that need to be accessed by multiple applications.

**Use Cases:**
- Create a combined NetworkPolicy that has the list of microservices allowed to connect to an application
- Share databases between multiple microservices in different tiers
- Allow multiple monitoring or logging tools to access application endpoints
- Enable cross-team service access with explicit whitelisting

## Requirements

### Task 1: Deploy Shared Database
**Priority:** High
**Status:** pending

Run a Redis database that will be shared by multiple microservices.

**Actions:**
- Deploy Redis pod with labels `app=bookstore` and `role=db`
- Expose service on port 6379
- Verify pod is running

**Command:**
```bash
kubectl run db --image=redis:4 --labels="app=bookstore,role=db" --expose --port=6379
```

### Task 2: Understand Microservice Architecture
**Priority:** High
**Status:** pending

Document the microservices that need access to the shared database.

**Microservice Labels:**

| Service    | Labels |
|------------|--------|
| `search`   | `app=bookstore`<br/>`role=search` |
| `api`      | `app=bookstore`<br/>`role=api` |
| `catalog`  | `app=inventory`<br/>`role=web` |

**Architecture:**
- All services need access to the Redis database
- Services have different label combinations
- Need to whitelist all three services explicitly

### Task 3: Create Multiple Selector Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that allows traffic from multiple pod selectors using OR logic.

**Actions:**
- Create `redis-allow-services.yaml` manifest
- Define multiple podSelector entries
- Apply policy to cluster

**Manifest:** `redis-allow-services.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: redis-allow-services
spec:
  podSelector:
    matchLabels:
      app: bookstore
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          app: bookstore
          role: search
    - podSelector:
        matchLabels:
          app: bookstore
          role: api
    - podSelector:
        matchLabels:
          app: inventory
          role: web
```

**Key Configuration:**
- Targets pods with `app=bookstore` and `role=db` labels
- Three separate podSelector entries (OR logic)
- Each selector specifies exact label combination
- All matching pods are whitelisted

**Command:**
```bash
kubectl apply -f redis-allow-services.yaml
```

**Expected Output:**
```
networkpolicy "redis-allow-services" created
```

**Important Notes:**
- Rules specified in `spec.ingress.from` are OR'ed
- This means the pods selected by the selectors are combined
- All selected pods are whitelisted altogether
- Any pod matching at least one selector can access the database

### Task 4: Test Access as Catalog Service (Allowed)
**Priority:** High
**Status:** pending

Verify that pods matching the catalog microservice labels can access the database.

**Actions:**
- Run test pod with labels `app=inventory,role=web`
- Connect to Redis database
- Verify connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --labels="app=inventory,role=web" --rm -i -t --image=alpine -- sh
# Inside the pod:
nc -v -w 2 db 6379
```

**Expected Result:**
```
db (10.59.242.200:6379) open
```

Connection works! Pod matches one of the allowed selectors.

### Task 5: Test Access as Search Service (Allowed)
**Priority:** High
**Status:** pending

Verify that pods matching the search microservice labels can access the database.

**Actions:**
- Run test pod with labels `app=bookstore,role=search`
- Connect to Redis database
- Verify connection succeeds

**Commands:**
```bash
kubectl run test-$RANDOM --labels="app=bookstore,role=search" --rm -i -t --image=alpine -- sh
# Inside the pod:
nc -v -w 2 db 6379
```

**Expected Result:**
```
db (10.59.242.200:6379) open
```

Connection works! Pod matches another allowed selector.

### Task 6: Test Access with Non-Whitelisted Labels (Blocked)
**Priority:** High
**Status:** pending

Verify that pods not matching any selector are blocked.

**Actions:**
- Run test pod with different labels `app=other`
- Attempt connection to database
- Confirm connection is blocked

**Commands:**
```bash
kubectl run test-$RANDOM --labels="app=other" --rm -i -t --image=alpine -- sh
# Inside the pod:
nc -v -w 2 db 6379
```

**Expected Result:**
```
nc: db (10.59.252.83:6379): Operation timed out
```

Connection blocked! Pod doesn't match any allowed selector.

## Acceptance Criteria

- [ ] Redis database deployed with labels `app=bookstore,role=db`
- [ ] Service exposed on port 6379
- [ ] NetworkPolicy `redis-allow-services` created successfully
- [ ] Policy contains three podSelector entries
- [ ] Pods with `app=bookstore,role=search` labels can connect
- [ ] Pods with `app=bookstore,role=api` labels can connect
- [ ] Pods with `app=inventory,role=web` labels can connect
- [ ] Pods with non-matching labels are blocked
- [ ] OR logic properly combines all selectors

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `redis-allow-services`
- Pod Selector: `app=bookstore` AND `role=db`
- Ingress Sources: Three pod selectors (OR'ed together)

**How It Works:**
- Multiple `from` entries create an OR condition
- A pod is allowed if it matches ANY of the selectors
- Each selector requires ALL its labels to match (AND within selector)
- Selectors are combined with OR logic (OR between selectors)

**OR Logic in Ingress Rules:**
```yaml
ingress:
- from:
  - podSelector: {...}  # Selector 1 (OR)
  - podSelector: {...}  # Selector 2 (OR)
  - podSelector: {...}  # Selector 3 (OR)
```

**Label Matching:**
- Within each podSelector: ALL labels must match (AND)
- Between podSelectors: ANY selector can match (OR)

**Example:**
```yaml
- podSelector:
    matchLabels:
      app: bookstore    # AND
      role: search      # Must have both labels
```

## Implementation Details

**Understanding OR vs AND:**

**OR between selectors (what we're using):**
```yaml
ingress:
- from:
  - podSelector:
      matchLabels:
        app: bookstore
        role: search
  - podSelector:
      matchLabels:
        app: bookstore
        role: api
# Allows: (bookstore+search) OR (bookstore+api)
```

**AND between selectors (different syntax):**
```yaml
ingress:
- from:
  - podSelector:
      matchLabels:
        app: bookstore
  - podSelector:
      matchLabels:
        role: search
# Allows: (bookstore) OR (search)
# Note: This is still OR, not AND!
```

**True AND requires same list item:**
```yaml
ingress:
- from:
  - namespaceSelector:
      matchLabels:
        team: operations
    podSelector:
      matchLabels:
        role: monitoring
# Allows: pods with role=monitoring in namespaces with team=operations
```

**Combining Multiple Services:**
```yaml
spec:
  podSelector:
    matchLabels:
      app: shared-cache
  ingress:
  - from:
    # Frontend services
    - podSelector:
        matchLabels:
          tier: frontend
          app: web
    - podSelector:
        matchLabels:
          tier: frontend
          app: mobile-api
    # Backend services
    - podSelector:
        matchLabels:
          tier: backend
          app: orders
    - podSelector:
        matchLabels:
          tier: backend
          app: inventory
```

**Using matchExpressions for More Flexible Matching:**
```yaml
ingress:
- from:
  - podSelector:
      matchExpressions:
      - key: app
        operator: In
        values: [bookstore, inventory]
      - key: role
        operator: In
        values: [api, search, web]
```

**Mixed Namespace and Pod Selectors:**
```yaml
ingress:
- from:
  # Allow from specific pods in same namespace
  - podSelector:
      matchLabels:
        app: service-a
  # Allow from specific pods in other namespace
  - namespaceSelector:
      matchLabels:
        environment: production
    podSelector:
      matchLabels:
        app: service-b
  # Allow from all pods in monitoring namespace
  - namespaceSelector:
      matchLabels:
        purpose: monitoring
```

## Verification

Check policy and test connectivity:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy redis-allow-services

# Check which pods are selected
kubectl get pods -l app=bookstore,role=db --show-labels

# Test each service label combination
# Test search service
kubectl run test-search --labels="app=bookstore,role=search" --rm -i -t --image=alpine -- sh
# Inside: nc -v -w 2 db 6379

# Test api service
kubectl run test-api --labels="app=bookstore,role=api" --rm -i -t --image=alpine -- sh
# Inside: nc -v -w 2 db 6379

# Test catalog service
kubectl run test-catalog --labels="app=inventory,role=web" --rm -i -t --image=alpine -- sh
# Inside: nc -v -w 2 db 6379

# Test unauthorized access
kubectl run test-blocked --labels="app=unauthorized" --rm -i -t --image=alpine -- sh
# Inside: nc -v -w 2 db 6379
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod db
kubectl delete service db
kubectl delete networkpolicy redis-allow-services
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Label Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- [matchExpressions](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement)

## Notes

**Best Practices:**
- Document which services need access and why
- Use consistent labeling schemes across microservices
- Group related selectors together with comments
- Consider using matchExpressions for large numbers of services
- Regularly audit and update allowed services list

**Common Patterns:**

**Shared Database Pattern:**
```yaml
# Allow multiple application tiers to access database
ingress:
- from:
  - podSelector:
      matchLabels:
        tier: frontend
  - podSelector:
      matchLabels:
        tier: backend
  - podSelector:
      matchLabels:
        tier: worker
```

**Multi-Team Access Pattern:**
```yaml
# Allow specific teams to access shared service
ingress:
- from:
  - podSelector:
      matchLabels:
        team: team-a
        role: api
  - podSelector:
      matchLabels:
        team: team-b
        role: api
  - podSelector:
      matchLabels:
        team: platform
        role: admin
```

**Environment-Based Pattern:**
```yaml
# Allow services from same environment only
ingress:
- from:
  - podSelector:
      matchExpressions:
      - key: environment
        operator: In
        values: [production]
      - key: access-level
        operator: In
        values: [privileged, standard]
```

**Using matchExpressions for Scalability:**
```yaml
# More maintainable for many services
ingress:
- from:
  - podSelector:
      matchExpressions:
      # Any service with these apps
      - key: app
        operator: In
        values: [service-a, service-b, service-c, service-d]
      # AND one of these roles
      - key: role
        operator: In
        values: [api, worker]
```

**Anti-Pattern - Too Broad:**
```yaml
# Avoid: This allows any pod with ANY of these labels
ingress:
- from:
  - podSelector:
      matchLabels:
        tier: frontend
  - podSelector:
      matchLabels:
        access: enabled
# Problem: Any pod with just 'access=enabled' can connect
```

**Debugging Tips:**
```bash
# Check if pod labels match selector
kubectl get pods --show-labels | grep "app=bookstore"

# Test label matching
kubectl get pods -l "app=bookstore,role=search"

# View effective labels on pod
kubectl describe pod <pod-name> | grep Labels

# Check NetworkPolicy details
kubectl get networkpolicy redis-allow-services -o yaml

# Test connectivity from specific pod
kubectl exec -it <pod-name> -- nc -zv db 6379
```

**Common Mistakes:**
- Confusing OR between selectors with AND within a selector
- Forgetting that all labels in matchLabels must match
- Not testing all combinations of allowed services
- Using too-broad selectors that allow unintended access
- Mixing up podSelector and namespaceSelector logic

**Scaling Considerations:**
- For 5+ services, consider using matchExpressions
- Document service access matrix
- Use automation to generate policies from service registry
- Consider using service mesh for very complex scenarios
- Monitor policy effectiveness with network flow logs

**Security Considerations:**
- Principle of least privilege: only allow necessary services
- Regularly review and audit allowed services
- Remove access for deprecated services
- Consider time-based access for temporary integrations
- Log and monitor database access patterns
- Combine with authentication at application level

This pattern is essential for implementing shared services in microservice architectures while maintaining security and explicit access control.
---
id: NP-11
title: Deny Egress Traffic from an Application
type: policy
category: egress
priority: high
status: ready
estimated_time: 25m
dependencies: [NP-00]
tags: [network-policy, egress, deny-all, outbound-traffic, dns]
---

## Overview

Implement a NetworkPolicy that prevents an application from establishing any outbound connections, useful for restricting egress traffic from single-instance databases and datastores.

## Objectives

- Block all egress (outbound) traffic from selected pods
- Understand DNS resolution requirements for network policies
- Implement selective DNS whitelisting
- Restrict outbound connections for security-sensitive applications

## Background

This NetworkPolicy denies all egress traffic from an application, preventing it from establishing any connections outside of the pod. This is useful for security-critical applications that should not initiate outbound connections.

**Use Cases:**
- Prevent applications from establishing connections outside of the pod
- Restrict outbound traffic of single-instance databases and datastores
- Implement data exfiltration protection
- Secure sensitive applications that should only respond to requests
- Prevent compromised applications from calling external services

**Important Notes:**
- If you are using Google Kubernetes Engine (GKE), make sure you have at least `1.8.4-gke.0` master and nodes version to be able to use egress policies
- Blocking all egress will also block DNS resolution unless explicitly allowed

## Requirements

### Task 1: Deploy Test Web Application
**Priority:** High
**Status:** pending

Run a web application for testing egress policies.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 2: Create Deny-All Egress Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that blocks all egress traffic.

**Actions:**
- Create `foo-deny-egress.yaml` manifest
- Configure policy to deny all outbound traffic
- Apply policy to cluster

**Manifest:** `foo-deny-egress.yaml`
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: foo-deny-egress
spec:
  podSelector:
    matchLabels:
      app: foo
  policyTypes:
  - Egress
  egress: []
```

**Key Configuration:**
- `podSelector` matches to `app=foo` pods
- `policyTypes: ["Egress"]` indicates this policy enforces egress (outbound) traffic
- `egress: []` empty rule set does not whitelist any traffic, therefore all egress traffic is blocked
- You can drop the egress field altogether and have the same effect

**Command:**
```bash
kubectl apply -f foo-deny-egress.yaml
```

**Expected Output:**
```
networkpolicy "foo-deny-egress" created
```

### Task 3: Test Egress Blocking Without DNS
**Priority:** High
**Status:** pending

Verify that egress traffic is blocked, including DNS resolution.

**Actions:**
- Run test pod with label `app=foo`
- Attempt connection to internal service
- Attempt connection to external service
- Observe DNS resolution failure

**Commands:**
```bash
kubectl run --rm --restart=Never --image=alpine -i -t --labels="app=foo" test -- ash
# Inside the pod:
wget -qO- --timeout 1 http://web:80/
wget -qO- --timeout 1 http://www.example.com/
```

**Expected Result:**
```
wget: bad address 'web:80'
wget: bad address 'www.example.com'
```

**What's Happening:**
- The pod is failing to resolve addresses
- Network policy is not allowing connections to kube-dns pods
- DNS resolution is blocked along with all other egress traffic

### Task 4: Update Policy to Allow DNS
**Priority:** High
**Status:** pending

Modify the policy to allow DNS resolution while blocking other traffic.

**Actions:**
- Update policy to allow DNS traffic to kube-dns
- Allow both UDP and TCP on port 53
- Use namespace and pod selectors to target kube-dns

**Updated Manifest:** `foo-deny-egress.yaml`
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: foo-deny-egress
spec:
  podSelector:
    matchLabels:
      app: foo
  policyTypes:
  - Egress
  egress:
  # allow DNS resolution
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP
```

**Key Changes:**
- Added egress rule for DNS
- Targets kube-system namespace
- Selects kube-dns pods specifically
- Allows both UDP and TCP on port 53
- All other traffic remains blocked

**Command:**
```bash
kubectl apply -f foo-deny-egress.yaml
```

### Task 5: Test Egress Blocking With DNS
**Priority:** High
**Status:** pending

Verify DNS resolution works but connections are still blocked.

**Actions:**
- Run test pod with label `app=foo`
- Verify DNS resolution succeeds
- Verify connections to resolved IPs are blocked
- Test both internal and external services

**Commands:**
```bash
kubectl run --rm --restart=Never --image=alpine -i -t --labels="app=foo" test -- ash
# Inside the pod:
wget --timeout 1 -O- http://web
```

**Expected Result:**
```
Connecting to web (10.59.245.232:80)
wget: download timed out
```

DNS resolution works (IP address shown), but connection is blocked!

**Test external service:**
```bash
# Inside the same pod:
wget --timeout 1 -O- http://www.example.com
```

**Expected Result:**
```
Connecting to www.example.com (93.184.216.34:80)
wget: download timed out
```

DNS resolution works, but connection is blocked!

**Test ping:**
```bash
# Inside the same pod:
ping google.com
```

**Expected Result:**
```
PING google.com (74.125.129.101): 56 data bytes
(no response, hit Ctrl+C to terminate)
```

DNS works, but ICMP traffic is blocked!

## Acceptance Criteria

- [ ] Web service deployed in cluster
- [ ] NetworkPolicy `foo-deny-egress` created successfully
- [ ] Policy targets pods with `app=foo` label
- [ ] Policy specifies Egress policyType
- [ ] Initial policy blocks all egress including DNS
- [ ] Updated policy allows DNS to kube-dns
- [ ] DNS resolution works for internal and external names
- [ ] Actual connections to resolved IPs are blocked
- [ ] Both TCP and ICMP traffic blocked

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `foo-deny-egress`
- Pod Selector: `app=foo`
- Policy Types: Egress
- Egress Rules: DNS only (to kube-dns in kube-system)

**How It Works:**
- `policyTypes: [Egress]` enforces egress policy
- Empty `egress: []` blocks all outbound traffic
- Adding DNS rule allows name resolution
- `namespaceSelector` + `podSelector` creates AND condition
- Only kube-dns pods in kube-system namespace are reachable
- All other egress traffic is denied

**Egress Policy Behavior:**
```yaml
# Deny all egress
egress: []

# Allow specific egress (DNS)
egress:
- to:
  - namespaceSelector: {...}
    podSelector: {...}
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
```

**DNS Resolution Flow:**
```
Pod (app=foo)
    ↓ (DNS query - allowed)
kube-dns (kube-system)
    ↓ (DNS response - allowed)
Pod (app=foo)
    ↓ (HTTP connection - blocked)
✗ Blocked by NetworkPolicy
```

## Implementation Details

**Understanding Egress Policies:**

**Complete Deny (no DNS):**
```yaml
policyTypes:
- Egress
egress: []  # Blocks everything including DNS
```

**Deny with DNS Exception:**
```yaml
policyTypes:
- Egress
egress:
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
```

**Why Both UDP and TCP for DNS:**
- DNS typically uses UDP for queries
- TCP is used for large responses (over 512 bytes)
- Zone transfers use TCP
- Both should be allowed for reliable DNS

**Alternative DNS Selectors:**
```yaml
# Using CoreDNS (common in newer clusters)
podSelector:
  matchLabels:
    k8s-app: kube-dns  # Works for both kube-dns and CoreDNS

# More specific CoreDNS selector
podSelector:
  matchLabels:
    k8s-app: kube-dns
    app.kubernetes.io/name: coredns
```

**Common Egress Patterns:**

**Allow Egress to Specific Services:**
```yaml
egress:
# DNS
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
# Specific internal service
- to:
  - podSelector:
      matchLabels:
        app: backend-api
  ports:
  - port: 8080
```

**Allow Egress to External IPs:**
```yaml
egress:
- to:
  - ipBlock:
      cidr: 10.0.0.0/8  # Internal network
- to:
  - ipBlock:
      cidr: 203.0.113.0/24  # Specific external range
  ports:
  - port: 443
```

## Verification

Check policy and test connectivity:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy foo-deny-egress

# Check kube-dns pods
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Test DNS resolution
kubectl run test-dns --labels="app=foo" --rm -i -t --image=alpine -- sh
# Inside:
nslookup kubernetes.default
nslookup google.com

# Test actual connectivity
kubectl run test-conn --labels="app=foo" --rm -i -t --image=alpine -- sh
# Inside:
wget -O- --timeout=2 http://web
wget -O- --timeout=2 http://www.example.com
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod,service web
kubectl delete networkpolicy foo-deny-egress
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Egress Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic)
- [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)

## Notes

**Best Practices:**
- Always allow DNS unless you have a specific reason not to
- Use specific namespace and pod selectors for DNS
- Test both DNS resolution and actual connectivity
- Document why egress is blocked for specific applications
- Monitor for legitimate egress requirements

**Common Use Cases:**

**Database with No Egress:**
```yaml
# PostgreSQL that only accepts connections
spec:
  podSelector:
    matchLabels:
      app: postgresql
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
```

**API Gateway with Limited Egress:**
```yaml
# Only allow egress to specific backend services
spec:
  podSelector:
    matchLabels:
      app: api-gateway
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
  # Backend services
  - to:
    - podSelector:
        matchLabels:
          tier: backend
```

**Debugging Tips:**
```bash
# Check if DNS is working
kubectl exec -it <pod> -- nslookup kubernetes.default

# Check if egress policy is applied
kubectl get networkpolicy -o yaml

# View kube-dns endpoint
kubectl get endpoints -n kube-system kube-dns

# Test DNS with dig
kubectl exec -it <pod> -- dig @<kube-dns-ip> google.com

# Check pod connectivity to DNS
kubectl exec -it <pod> -- nc -zv <kube-dns-ip> 53
```

**Common Mistakes:**
- Forgetting to allow DNS (leads to name resolution failures)
- Only allowing UDP for DNS (TCP needed for large responses)
- Not using both namespace and pod selectors for DNS
- Assuming the policy blocks ingress (it only blocks egress)
- Blocking legitimate health checks or monitoring

**Security Considerations:**
- Egress policies are defense-in-depth
- Don't rely solely on network policies for security
- Combine with:
  - Container security policies
  - Application-level security
  - Network segmentation
  - Monitoring and alerting
- Regularly review egress requirements
- Monitor for policy violations

**GKE Specific Notes:**
- Requires GKE version 1.8.4-gke.0 or later
- Egress policies may have performance impact
- Test thoroughly before production deployment
- Check GKE release notes for CNI plugin updates

**DNS Configuration Notes:**
- Different clusters may use different DNS labels
- CoreDNS is common in newer clusters
- Check your cluster's DNS pod labels:
  ```bash
  kubectl get pods -n kube-system -l k8s-app=kube-dns --show-labels
  ```
- Adjust the policy to match your DNS pod labels

**Performance Considerations:**
- Egress policies can impact network performance
- Test application performance after applying policies
- Monitor for increased latency
- Consider the overhead of policy evaluation

This pattern is essential for implementing zero-trust networking and preventing data exfiltration from sensitive applications.
---
id: NP-12
title: Deny All Non-Whitelisted Egress Traffic from a Namespace
type: policy
category: egress
priority: high
status: ready
estimated_time: 15m
dependencies: [NP-00]
tags: [network-policy, egress, default-deny, namespace-wide, dns-blocking]
---

## Overview

Implement a fundamental default-deny egress policy that blocks all outgoing traffic from a namespace by default, including DNS resolution, forcing explicit whitelisting of all egress traffic.

## Objectives

- Create a namespace-wide default-deny egress policy
- Block all outgoing traffic including DNS by default
- Establish foundation for explicit egress whitelisting
- Understand the importance of egress control in zero-trust networks

## Background

This is a fundamental policy that blocks all outgoing (egress) traffic from a namespace by default, including DNS resolution. After deploying this, you can deploy Network Policies that allow specific outgoing traffic.

**Use Cases:**
- Implement default "deny all" egress functionality for namespaces
- Create clear visibility into component dependencies
- Deploy network policies that can be translated to dependency graphs
- Enforce zero-trust networking principles
- Prevent unauthorized data exfiltration

**Best Practice:** This policy will give you a default "deny all" functionality. This way, you can clearly identify which components have dependency on which components and deploy Network Policies that can be translated to dependency graphs between components.

**Important:** Consider applying this manifest to any namespace you deploy workloads to (except `kube-system`).

## Requirements

### Task 1: Understand the Policy
**Priority:** High
**Status:** pending

Review and understand the default-deny egress policy structure.

**Policy Characteristics:**
- Applies to entire namespace
- Blocks all egress traffic
- Includes DNS resolution
- Forces explicit whitelisting
- Foundation for zero-trust networking

### Task 2: Create Default-Deny Egress Policy
**Priority:** High
**Status:** pending

Create and apply the namespace-wide egress deny policy.

**Actions:**
- Create `default-deny-all-egress.yaml` manifest
- Configure to target all pods in namespace
- Apply to default namespace

**Manifest:** `default-deny-all-egress.yaml`
```yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: default-deny-all-egress
  namespace: default
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress: []
```

**Key Configuration:**
- `namespace: default` - Deploy to the default namespace
- `podSelector: {}` - Empty selector matches ALL pods
- `policyTypes: [Egress]` - Only affects egress traffic
- `egress: []` - Empty array blocks all egress traffic

**Important Notes:**
- `podSelector: {}` (empty) matches all pods in the namespace
- Empty `egress` list causes all traffic to be dropped
- This includes DNS resolution
- Pods will be unable to make any outbound connections

**Command:**
```bash
kubectl apply -f default-deny-all-egress.yaml
```

**Expected Output:**
```
networkpolicy "default-deny-all-egress" created
```

### Task 3: Test Egress Blocking
**Priority:** High
**Status:** pending

Verify that all egress traffic is blocked, including DNS.

**Actions:**
- Deploy test pod in the namespace
- Attempt internal service connection
- Attempt external connection
- Verify DNS resolution fails

**Test Commands:**
```bash
# Deploy test pod
kubectl run test-$RANDOM --rm -i -t --image=alpine -- sh

# Inside the pod, test internal service
wget -qO- --timeout=2 http://kubernetes.default

# Inside the pod, test external service
wget -qO- --timeout=2 http://www.example.com
```

**Expected Result:**
```
wget: bad address 'kubernetes.default'
wget: bad address 'www.example.com'
```

All egress traffic is blocked, including DNS!

## Acceptance Criteria

- [ ] NetworkPolicy `default-deny-all-egress` created successfully
- [ ] Policy deployed to default namespace
- [ ] Policy uses empty podSelector (matches all pods)
- [ ] Policy specifies Egress policyType
- [ ] Empty egress array blocks all traffic
- [ ] DNS resolution blocked
- [ ] Internal service connections blocked
- [ ] External connections blocked
- [ ] Policy provides foundation for whitelisting

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `default-deny-all-egress`
- Namespace: `default`
- Pod Selector: `{}` (all pods)
- Policy Types: Egress
- Egress Rules: None (empty array)

**How It Works:**
- Empty `podSelector: {}` matches every pod in the namespace
- `policyTypes: [Egress]` enforces egress policy
- Empty `egress: []` array blocks all outbound traffic
- No exceptions - even DNS is blocked
- Creates foundation for explicit whitelisting

**Policy Scope:**
```yaml
podSelector: {}  # Matches ALL pods
egress: []       # Allows NO traffic
```

**Effect:**
```
Pod → Any Destination = ✗ BLOCKED
Pod → DNS Server = ✗ BLOCKED
Pod → Internal Service = ✗ BLOCKED
Pod → External IP = ✗ BLOCKED
```

## Implementation Details

**Understanding Default-Deny Egress:**

**Complete Lockdown:**
```yaml
spec:
  policyTypes:
  - Egress
  podSelector: {}  # All pods in namespace
  egress: []       # No traffic allowed
```

**Why This Is Important:**
- Forces explicit documentation of dependencies
- Prevents unauthorized egress traffic
- Enables dependency mapping
- Implements zero-trust principles
- Provides clear audit trail

**Namespace Selection Best Practices:**
```yaml
# Apply to specific namespace
metadata:
  name: default-deny-all-egress
  namespace: default  # Change this for each namespace

# DO NOT apply to:
# - kube-system (system components need egress)
# - Monitoring namespaces (metrics collection needs egress)
# - Ingress controller namespaces
```

**Building on This Foundation:**

After applying this policy, create additional policies to allow specific traffic:

**Example: Allow DNS**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: default
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```

**Example: Allow Specific Service Access**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-allow-backend
  namespace: default
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - port: 8080
```

**Example: Allow External API Access**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-allow-external-api
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: myapp
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 0.0.0.0/0
        except:
        - 169.254.169.254/32  # Block metadata service
    ports:
    - port: 443
      protocol: TCP
```

**Policy Interaction:**
- NetworkPolicies are additive
- Multiple policies are OR'ed together
- Default-deny + specific allow = whitelisting
- Each component gets its own allow policy

## Verification

Check policy and test effectiveness:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy default-deny-all-egress

# Check which pods are affected
kubectl get pods --all-namespaces

# Test from any pod in the namespace
kubectl run test --rm -i -t --image=alpine -- sh
# Inside:
ping 8.8.8.8  # Should fail
nslookup google.com  # Should fail
wget http://kubernetes.default  # Should fail

# View all network policies in namespace
kubectl get networkpolicy -n default
```

## Cleanup

### Task: Remove Policy
Remove the default-deny egress policy:

```bash
kubectl delete networkpolicy default-deny-all-egress
```

**Warning:** Only remove this policy if you're certain no other policies depend on it for security.

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Default Deny Egress](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic)
- [Zero Trust Networking](https://kubernetes.io/blog/2021/04/05/network-policies-conformance-cni/)

## Notes

**Best Practices:**
- Apply to all workload namespaces (except system namespaces)
- Create this policy first, then add allow policies
- Document all egress requirements before deployment
- Test thoroughly in non-production environments
- Monitor for blocked legitimate traffic

**Namespace Application Strategy:**
```bash
# Apply to multiple namespaces
for ns in production staging development; do
  cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all-egress
  namespace: $ns
spec:
  policyTypes:
  - Egress
  podSelector: {}
  egress: []
EOF
done
```

**Common Patterns After Default-Deny:**

**Pattern 1: DNS for All Pods**
```yaml
# Allow DNS for all pods in namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns-access
spec:
  podSelector: {}
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```

**Pattern 2: Internal Services Only**
```yaml
# Allow internal cluster services only
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-internal-only
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector: {}  # All pods in same namespace
  - to:
    - namespaceSelector: {}  # All pods in all namespaces
```

**Pattern 3: Specific External Services**
```yaml
# Allow only specific external IPs
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-specific-external
spec:
  podSelector:
    matchLabels:
      app: api-client
  policyTypes:
  - Egress
  egress:
  - to:
    - ipBlock:
        cidr: 203.0.113.0/24  # Specific external service
    ports:
    - port: 443
```

**Debugging Tips:**
```bash
# Check if policy is applied
kubectl get networkpolicy -n default

# View policy details
kubectl describe networkpolicy default-deny-all-egress

# Test egress from specific pod
kubectl exec -it <pod-name> -- wget -qO- --timeout=2 http://google.com

# Check DNS resolution
kubectl exec -it <pod-name> -- nslookup kubernetes.default

# View all policies affecting a pod
kubectl get networkpolicy -n default -o yaml
```

**Common Mistakes:**
- Applying to kube-system namespace (breaks cluster)
- Not allowing DNS in follow-up policies
- Forgetting about health checks and monitoring
- Not testing before production
- Blocking legitimate inter-service communication

**Exceptions and Special Cases:**

**DO NOT Apply To:**
- `kube-system` namespace (system pods need egress)
- `kube-public` namespace
- Monitoring namespaces (Prometheus, Grafana, etc.)
- Logging namespaces (Fluentd, Logstash, etc.)
- Ingress controller namespaces

**Considerations:**
- Health checks may need egress
- Monitoring agents need to reach collection endpoints
- Logging agents need to send logs
- Init containers may need external resources
- Application startup may require external configuration

**Progressive Rollout:**
```bash
# 1. Start with audit/monitoring only
#    Deploy but don't enforce - log violations

# 2. Apply to dev namespace first
kubectl apply -f default-deny-all-egress.yaml -n development

# 3. Identify required egress (check logs)
kubectl logs <pod> | grep "connection refused\|timeout"

# 4. Create allow policies
kubectl apply -f allow-dns.yaml -n development
kubectl apply -f allow-backend.yaml -n development

# 5. Test thoroughly
# Run integration tests

# 6. Roll out to staging, then production
kubectl apply -f default-deny-all-egress.yaml -n staging
kubectl apply -f default-deny-all-egress.yaml -n production
```

**Security Considerations:**
- This is defense-in-depth, not a silver bullet
- Combine with Pod Security Policies/Standards
- Use alongside RBAC and authentication
- Monitor for policy violations
- Regular security audits
- Document all allow policies and their justification

**Monitoring and Alerting:**
```bash
# Monitor for blocked connections (if your CNI supports it)
# Example with Cilium:
cilium monitor --type drop

# Example with Calico:
calicoctl get globalnetworkpolicy -o yaml

# Check for policy violations in logs
kubectl logs -n kube-system -l k8s-app=calico-node | grep "denied"
```

**Documentation Template:**
```yaml
# Document why egress is needed
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: app-allow-database
  namespace: production
  annotations:
    description: "Allow frontend to connect to PostgreSQL"
    jira-ticket: "SEC-1234"
    approved-by: "security-team"
    approved-date: "2025-01-15"
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Egress
  egress:
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
    ports:
    - port: 5432
```

This policy is essential for implementing zero-trust networking and maintaining clear visibility into service dependencies and data flows within your Kubernetes cluster.
---
id: NP-13
title: Allow Egress Traffic to Specific Pods
type: policy
category: egress
priority: high
status: ready
estimated_time: 25m
dependencies: [NP-00, NP-11, NP-12]
tags: [network-policy, egress, pod-selector, selective-egress, microservices]
---

## Overview

Implement a NetworkPolicy that allows an application to establish outbound connections only to specific pods, enabling fine-grained egress control for microservices architectures.

## Objectives

- Allow egress traffic only to specific pods using label selectors
- Implement selective egress control for microservices
- Whitelist DNS resolution alongside specific pod access
- Understand egress policy patterns for service dependencies

## Background

This NetworkPolicy allows an application to establish outbound connections only to specific pods in the cluster. This is useful for implementing the principle of least privilege in microservices architectures, where services should only communicate with their direct dependencies.

**Use Cases:**
- Restrict frontend applications to communicate only with specific backend APIs
- Allow worker pods to connect only to designated message queues
- Limit application egress to specific database pods
- Implement strict service dependency graphs
- Prevent lateral movement in compromised applications

**Important Notes:**
- Egress policies require Kubernetes v1.8+ (GKE 1.8.4-gke.0+)
- Always whitelist DNS unless you use IP addresses directly
- Combine with ingress policies for comprehensive network security

![Diagram of ALLOW egress traffic to specific pods policy](img/13.gif)

## Requirements

### Task 1: Deploy Backend API Service
**Priority:** High
**Status:** pending

Deploy a backend API service that will be the target of allowed egress traffic.

**Actions:**
- Deploy nginx as backend API with label `app=backend-api`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run backend-api --image=nginx --labels="app=backend-api,role=api" --expose --port=80
```

### Task 2: Deploy Frontend Application
**Priority:** High
**Status:** pending

Deploy a frontend application that will have restricted egress.

**Actions:**
- Deploy nginx as frontend with label `app=frontend`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run frontend --image=nginx --labels="app=frontend,role=web" --expose --port=80
```

### Task 3: Test Initial Connectivity (Before Policy)
**Priority:** High
**Status:** pending

Verify egress connectivity works before applying restrictions.

**Actions:**
- Exec into frontend pod
- Test connectivity to backend-api
- Test connectivity to external services
- Confirm DNS resolution works

**Commands:**
```bash
# Get frontend pod name
kubectl get pods -l app=frontend

# Test connectivity to backend
kubectl exec -it frontend -- wget -qO- --timeout=2 http://backend-api
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
```

Connectivity works without restrictions.

**Test external connectivity:**
```bash
kubectl exec -it frontend -- wget -qO- --timeout=2 http://www.example.com
```

Should succeed (internet access works).

### Task 4: Create Selective Egress Policy
**Priority:** High
**Status:** pending

Create NetworkPolicy that allows egress only to DNS and specific backend pods.

**Actions:**
- Create `frontend-allow-egress-to-backend.yaml` manifest
- Configure egress to kube-dns for DNS resolution
- Configure egress to backend-api pods only
- Apply policy to cluster

**Manifest:** `frontend-allow-egress-to-backend.yaml`
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-allow-egress-to-backend
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Egress
  egress:
  # Allow DNS resolution
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Allow egress to backend-api pods
  - to:
    - podSelector:
        matchLabels:
          app: backend-api
    ports:
    - port: 80
      protocol: TCP
```

**Key Configuration:**
- `podSelector` targets pods with `app=frontend` label
- `policyTypes: [Egress]` enforces egress policy
- First egress rule: Allow DNS (UDP/TCP port 53 to kube-dns)
- Second egress rule: Allow HTTP (TCP port 80 to backend-api pods)
- All other egress traffic is denied

**Command:**
```bash
kubectl apply -f frontend-allow-egress-to-backend.yaml
```

**Expected Output:**
```
networkpolicy.networking.k8s.io/frontend-allow-egress-to-backend created
```

### Task 5: Verify DNS Resolution Works
**Priority:** High
**Status:** pending

Confirm DNS resolution is functional after policy application.

**Actions:**
- Test DNS resolution for internal services
- Test DNS resolution for external domains
- Verify DNS queries succeed

**Commands:**
```bash
kubectl exec -it frontend -- nslookup backend-api
```

**Expected Result:**
```
Server:		10.96.0.10
Address:	10.96.0.10:53

Name:	backend-api.default.svc.cluster.local
Address: 10.100.200.50
```

DNS resolution works!

**Test external DNS:**
```bash
kubectl exec -it frontend -- nslookup google.com
```

**Expected Result:**
```
Server:		10.96.0.10
Address:	10.96.0.10:53

Non-authoritative answer:
Name:	google.com
Address: 142.250.185.46
```

DNS resolution works for external domains too!

### Task 6: Verify Allowed Egress to Backend
**Priority:** High
**Status:** pending

Confirm egress traffic to backend-api is allowed.

**Actions:**
- Connect from frontend to backend-api
- Verify successful HTTP response
- Confirm DNS resolution and connection both work

**Command:**
```bash
kubectl exec -it frontend -- wget -qO- --timeout=2 http://backend-api
```

**Expected Result:**
```html
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
```

Connection to backend-api is allowed!

### Task 7: Verify Blocked Egress to External Services
**Priority:** High
**Status:** pending

Confirm egress traffic to external services is blocked.

**Actions:**
- Attempt connection to external service
- Verify DNS resolves but connection times out
- Confirm egress blocking is working

**Command:**
```bash
kubectl exec -it frontend -- wget -qO- --timeout=2 http://www.example.com
```

**Expected Result:**
```
Connecting to www.example.com (93.184.216.34:80)
wget: download timed out
```

DNS resolution works, but connection is blocked!

### Task 8: Verify Blocked Egress to Other Pods
**Priority:** High
**Status:** pending

Deploy another pod and confirm egress to it is blocked.

**Actions:**
- Deploy another service (e.g., database)
- Attempt connection from frontend
- Verify connection is blocked

**Commands:**
```bash
# Deploy database pod
kubectl run database --image=nginx --labels="app=database" --expose --port=80

# Try to connect from frontend
kubectl exec -it frontend -- wget -qO- --timeout=2 http://database
```

**Expected Result:**
```
Connecting to database (10.100.200.75:80)
wget: download timed out
```

Egress to non-whitelisted pods is blocked!

## Acceptance Criteria

- [ ] Backend API service deployed with label `app=backend-api`
- [ ] Frontend application deployed with label `app=frontend`
- [ ] NetworkPolicy `frontend-allow-egress-to-backend` created successfully
- [ ] Policy specifies Egress policyType
- [ ] DNS resolution works for internal and external names
- [ ] Egress to backend-api pods is allowed
- [ ] Egress to external services is blocked (DNS resolves, connection times out)
- [ ] Egress to other internal pods is blocked
- [ ] Only DNS and backend-api traffic is permitted

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `frontend-allow-egress-to-backend`
- Pod Selector: `app=frontend`
- Policy Types: Egress
- Egress Rules:
  1. DNS to kube-dns (UDP/TCP port 53)
  2. HTTP to backend-api pods (TCP port 80)

**How It Works:**
- `policyTypes: [Egress]` activates egress enforcement
- Multiple egress rules are evaluated with OR logic
- Traffic matching ANY rule is allowed
- Traffic matching NO rule is blocked
- DNS must be explicitly whitelisted
- Pod selectors identify allowed destination pods

**Egress Rule Behavior:**
```yaml
# Rule 1: Allow DNS
egress:
- to:
  - namespaceSelector: {...}  # kube-system
    podSelector: {...}         # kube-dns
  ports:
  - port: 53

# Rule 2: Allow backend access
- to:
  - podSelector: {...}  # backend-api
  ports:
  - port: 80
```

**Traffic Flow:**
```
Frontend Pod
    ↓ DNS query (port 53)
kube-dns (allowed by rule 1)
    ↓ DNS response
Frontend Pod
    ↓ HTTP request (port 80)
Backend-api Pod (allowed by rule 2)
    ↓ HTTP response
Frontend Pod
    ↓ HTTP request (port 80)
✗ External Service (blocked - no matching rule)
```

## Implementation Details

**Understanding Selective Egress:**

**Complete Egress Allow:**
```yaml
# No egress field = all egress allowed (default)
spec:
  podSelector:
    matchLabels:
      app: frontend
# No policyTypes means only ingress is enforced
```

**Complete Egress Deny:**
```yaml
# Empty egress array blocks everything
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Egress
  egress: []
```

**Selective Egress (DNS + Specific Pods):**
```yaml
spec:
  podSelector:
    matchLabels:
      app: frontend
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Specific pods
  - to:
    - podSelector:
        matchLabels:
          app: backend-api
    ports:
    - port: 80
      protocol: TCP
```

**Multiple Egress Destinations:**
```yaml
egress:
# DNS
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
# Backend API
- to:
  - podSelector:
      matchLabels:
        app: backend-api
  ports:
  - port: 80
# Message Queue
- to:
  - podSelector:
      matchLabels:
        app: rabbitmq
  ports:
  - port: 5672
# Database
- to:
  - podSelector:
      matchLabels:
        app: postgresql
  ports:
  - port: 5432
```

**Cross-Namespace Egress:**
```yaml
# Allow egress to pods in specific namespace
egress:
- to:
  - namespaceSelector:
      matchLabels:
        name: backend-services
    podSelector:
      matchLabels:
        app: api
  ports:
  - port: 8080
```

**Combining Pod and Namespace Selectors:**
```yaml
# AND condition: specific pods in specific namespace
- to:
  - namespaceSelector:
      matchLabels:
        environment: production
    podSelector:
      matchLabels:
        app: backend
  ports:
  - port: 8080
```

## Real-World Examples

### Example 1: Frontend to Backend API Pattern
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: frontend-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      tier: frontend
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Backend API (same namespace)
  - to:
    - podSelector:
        matchLabels:
          tier: backend
          app: api-server
    ports:
    - port: 8080
      protocol: TCP
  # Authentication service (different namespace)
  - to:
    - namespaceSelector:
        matchLabels:
          name: auth-services
      podSelector:
        matchLabels:
          app: auth-server
    ports:
    - port: 9090
      protocol: TCP
```

### Example 2: Microservice with Database Access
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-service-egress
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: api-service
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # PostgreSQL database
  - to:
    - podSelector:
        matchLabels:
          app: postgresql
          role: primary
    ports:
    - port: 5432
      protocol: TCP
  # Redis cache
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - port: 6379
      protocol: TCP
```

### Example 3: Worker Pod with Message Queue
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: worker-egress
  namespace: jobs
spec:
  podSelector:
    matchLabels:
      app: background-worker
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # RabbitMQ
  - to:
    - namespaceSelector:
        matchLabels:
          name: messaging
      podSelector:
        matchLabels:
          app: rabbitmq
    ports:
    - port: 5672
      protocol: TCP
  # S3 API endpoint (for file storage)
  - to:
    - podSelector:
        matchLabels:
          app: minio
    ports:
    - port: 9000
      protocol: TCP
```

### Example 4: Multi-tier Application
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: webapp-egress
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: webapp
      tier: middle
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Backend services (same namespace)
  - to:
    - podSelector:
        matchLabels:
          tier: backend
    ports:
    - port: 8080
      protocol: TCP
  # Monitoring (metrics export)
  - to:
    - namespaceSelector:
        matchLabels:
          name: monitoring
      podSelector:
        matchLabels:
          app: prometheus
    ports:
    - port: 9090
      protocol: TCP
```

## Debugging Tips

**Check Policy Configuration:**
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy frontend-allow-egress-to-backend

# View policy in YAML
kubectl get networkpolicy frontend-allow-egress-to-backend -o yaml
```

**Verify Pod Labels:**
```bash
# Check frontend pod labels
kubectl get pods -l app=frontend --show-labels

# Check backend pod labels
kubectl get pods -l app=backend-api --show-labels

# Check all pod labels in namespace
kubectl get pods --show-labels
```

**Test DNS Resolution:**
```bash
# Test internal DNS
kubectl exec -it frontend -- nslookup backend-api

# Test external DNS
kubectl exec -it frontend -- nslookup google.com

# Check DNS server
kubectl exec -it frontend -- cat /etc/resolv.conf
```

**Test Connectivity:**
```bash
# Test with wget
kubectl exec -it frontend -- wget -qO- --timeout=2 http://backend-api

# Test with curl
kubectl exec -it frontend -- curl -m 2 http://backend-api

# Test with netcat
kubectl exec -it frontend -- nc -zv backend-api 80

# Test with telnet
kubectl exec -it frontend -- telnet backend-api 80
```

**Check kube-dns:**
```bash
# Find kube-dns pods
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Check kube-dns labels
kubectl get pods -n kube-system -l k8s-app=kube-dns --show-labels

# Test DNS directly
kubectl exec -it frontend -- dig @<kube-dns-ip> backend-api
```

**Verify Service Endpoints:**
```bash
# Check backend service
kubectl get service backend-api

# Check service endpoints
kubectl get endpoints backend-api

# Describe service
kubectl describe service backend-api
```

## Verification

Comprehensive verification checklist:

```bash
# 1. Verify policy exists and targets correct pods
kubectl get networkpolicy
kubectl describe networkpolicy frontend-allow-egress-to-backend

# 2. Check pod labels match policy selectors
kubectl get pods -l app=frontend --show-labels
kubectl get pods -l app=backend-api --show-labels

# 3. Test DNS resolution (should work)
kubectl exec -it frontend -- nslookup backend-api
kubectl exec -it frontend -- nslookup google.com

# 4. Test allowed egress (should succeed)
kubectl exec -it frontend -- wget -qO- --timeout=2 http://backend-api

# 5. Test blocked egress to external (should timeout)
kubectl exec -it frontend -- wget -qO- --timeout=2 http://www.example.com

# 6. Test blocked egress to other pods (should timeout)
kubectl run other-service --image=nginx --expose --port=80
kubectl exec -it frontend -- wget -qO- --timeout=2 http://other-service

# 7. Verify from non-policy pod (should work unrestricted)
kubectl run test-pod --rm -i -t --image=alpine -- sh
# Inside: wget -qO- http://www.example.com
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod frontend backend-api database
kubectl delete service frontend backend-api database
kubectl delete networkpolicy frontend-allow-egress-to-backend
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Egress Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic)
- [DNS for Services and Pods](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/)
- [Label Selectors](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)

## Notes

**Best Practices:**
- Always explicitly allow DNS for service name resolution
- Document service dependencies for each microservice
- Use consistent labeling schemes across applications
- Test both positive and negative cases
- Start with deny-all, then whitelist necessary traffic
- Combine with ingress policies for comprehensive security
- Regular audit of egress policies

**Common Use Cases:**

**Three-Tier Application:**
```
Frontend → Backend API → Database
  (NP)   →    (NP)    →   (NP)

Frontend egress: DNS + Backend API
Backend egress: DNS + Database
Database egress: DNS only (or deny all)
```

**Microservices Architecture:**
```
Gateway → [Service A, Service B, Service C] → Database
  (NP)   →         (NP)                      →   (NP)

Gateway egress: DNS + ServiceA + ServiceB + ServiceC
ServiceA egress: DNS + ServiceB + Database
ServiceB egress: DNS + ServiceC + Database
ServiceC egress: DNS + Database
```

**Batch Processing:**
```
Scheduler → Worker Pods → Message Queue → Database
   (NP)   →     (NP)    →      (NP)      →   (NP)

Scheduler egress: DNS + Workers + MessageQueue
Workers egress: DNS + MessageQueue + Database
MessageQueue egress: DNS only
Database egress: DNS only (or deny all)
```

**Common Mistakes:**
- Forgetting to allow DNS (leads to service resolution failures)
- Using service ports instead of pod ports
- Not testing DNS resolution separately from connectivity
- Assuming egress policies block ingress (they don't)
- Forgetting namespace selectors for cross-namespace communication
- Not documenting service dependencies
- Over-permissive selectors (e.g., allowing all pods with certain label)

**Troubleshooting Common Issues:**

**Issue: DNS not working**
```bash
# Check if DNS is in the egress rules
kubectl get networkpolicy <name> -o yaml | grep -A 10 egress

# Verify kube-dns pods exist
kubectl get pods -n kube-system -l k8s-app=kube-dns

# Test DNS directly
kubectl exec -it <pod> -- nslookup kubernetes.default
```

**Issue: Connection to allowed pod fails**
```bash
# Verify backend pod exists and has correct labels
kubectl get pods -l app=backend-api --show-labels

# Check if backend service exists
kubectl get service backend-api

# Verify pod is listening on specified port
kubectl exec -it backend-api -- netstat -tlnp

# Check if NetworkPolicy has correct podSelector
kubectl get networkpolicy <name> -o yaml | grep -A 5 podSelector
```

**Issue: Egress still allowed to blocked destinations**
```bash
# Check if policy exists and targets correct pods
kubectl describe networkpolicy <name>

# Verify pod has matching labels
kubectl get pod <name> --show-labels

# Check if there are other NetworkPolicies that might be allowing traffic
kubectl get networkpolicy -o yaml

# Check CNI plugin logs for policy enforcement
kubectl logs -n kube-system -l k8s-app=calico-node
```

**Security Considerations:**
- Egress policies implement defense-in-depth
- Should be combined with:
  - Ingress NetworkPolicies
  - Pod Security Standards/Admission
  - Service mesh mTLS
  - Application-level authentication
  - Network segmentation
  - Monitoring and alerting
- Egress policies prevent:
  - Unauthorized external communication
  - Lateral movement within cluster
  - Data exfiltration
  - Compromised pods calling out to C&C servers
- Egress policies DON'T prevent:
  - Attacks within allowed traffic flows
  - Application-level vulnerabilities
  - Privilege escalation
  - Host-level attacks

**Performance Considerations:**
- Egress policies add minimal overhead
- Evaluated at CNI plugin level
- Large numbers of rules can impact performance
- Test performance with realistic workloads
- Monitor for increased latency
- Consider rule complexity vs. security benefit

**GKE Specific Notes:**
- Requires GKE version 1.8.4-gke.0 or later
- Network Policy must be enabled on cluster
- Uses Calico for policy enforcement
- Check GKE release notes for feature updates
- Test policies after GKE upgrades

**Alternative Approaches:**

**Using Named Ports:**
```yaml
# In pod specification
ports:
- name: http
  containerPort: 8080
- name: metrics
  containerPort: 9090

# In NetworkPolicy
egress:
- to:
  - podSelector:
      matchLabels:
        app: backend
  ports:
  - port: http  # References named port
```

**Using IP Blocks (for external services):**
```yaml
egress:
# Allow specific external IP range
- to:
  - ipBlock:
      cidr: 203.0.113.0/24
  ports:
  - port: 443
    protocol: TCP
```

**Combining Multiple Conditions:**
```yaml
egress:
# Allow to specific pods in specific namespace on specific ports
- to:
  - namespaceSelector:
      matchLabels:
        environment: production
    podSelector:
      matchLabels:
        app: api-server
  ports:
  - port: 8080
    protocol: TCP
  - port: 8443
    protocol: TCP
```

This pattern is essential for implementing least-privilege networking in microservices architectures, where services should only communicate with their direct dependencies.
---
id: NP-14
title: Deny External Egress Traffic
type: policy
category: egress
priority: high
status: ready
estimated_time: 20m
dependencies: [NP-00]
tags: [network-policy, egress, external-traffic, cluster-internal, dns]
---

## Overview

Implement a NetworkPolicy that limits egress traffic to pods within the cluster only, preventing applications from establishing connections to external networks while allowing internal cluster communication.

## Objectives

- Block external egress traffic while allowing internal cluster communication
- Allow DNS resolution for service discovery
- Restrict applications from accessing external networks
- Implement internal-only network segmentation

## Background

This NetworkPolicy prevents applications from establishing connections to external networks while allowing communication with other pods in the cluster. This is also known as limiting traffic to pods in the cluster.

**Use Cases:**
- Prevent certain types of applications from establishing connections to external networks
- Restrict data exfiltration to external services
- Enforce internal-only communication for sensitive services
- Implement network segmentation between internal and external traffic
- Secure backend services that should only communicate internally

**Important Notes:**
- If you are using Google Kubernetes Engine (GKE), make sure you have at least `1.8.4-gke.0` master and nodes version to be able to use egress policies
- This policy allows internal cluster traffic but blocks external destinations

## Requirements

### Task 1: Create Policy Blocking External Egress
**Priority:** High
**Status:** pending

Create NetworkPolicy that allows internal cluster traffic and DNS but blocks external egress.

**Actions:**
- Create `foo-deny-external-egress.yaml` manifest
- Configure DNS access to kube-dns
- Block all external IP addresses
- Apply policy to cluster

**Manifest:** `foo-deny-external-egress.yaml`
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: foo-deny-external-egress
spec:
  podSelector:
    matchLabels:
      app: foo
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
      - port: 53
        protocol: UDP
      - port: 53
        protocol: TCP
```

**Key Configuration:**
- `podSelector` targets pods with `app=foo` label
- `policyTypes: [Egress]` enforces egress policy
- Single egress rule allows DNS to kube-dns
- No other egress rules means only DNS is allowed
- Implicit deny for all other egress traffic

**Important Notes:**
- This policy applies to pods with `app=foo` and affects Egress (outbound) direction
- Similar to NP-11 (deny egress from application), this policy allows all outbound traffic on ports 53/udp and 53/tcp to kube-dns pods for DNS resolution
- The `to` section specifies a `namespaceSelector` which matches `kubernetes.io/metadata.name: kube-system` and a `podSelector` which matches `k8s-app: kube-dns`
- This will select only the kube-dns pods in the kube-system namespace, so outbound traffic to kube-dns pods will be allowed
- Since IP addresses outside the cluster are not listed, traffic to external IPs is denied

**Command:**
```bash
kubectl apply -f foo-deny-external-egress.yaml
```

**Expected Output:**
```
networkpolicy "foo-deny-egress" created
```

### Task 2: Deploy Internal Web Service
**Priority:** High
**Status:** pending

Run a web application to test internal connectivity.

**Actions:**
- Deploy nginx pod with label `app=web`
- Expose service on port 80
- Verify pod is running

**Command:**
```bash
kubectl run web --image=nginx --labels="app=web" --expose --port=80
```

### Task 3: Test Internal Cluster Communication (Allowed)
**Priority:** High
**Status:** pending

Verify that pods can communicate with internal services.

**Actions:**
- Run test pod with label `app=foo`
- Connect to internal web service
- Verify connection succeeds

**Commands:**
```bash
kubectl run --rm --restart=Never --image=alpine -i -t --labels="app=foo" test -- ash
# Inside the pod:
wget -O- --timeout 1 http://web:80
```

**Expected Result:**
```
Connecting to web (10.59.245.232:80)
<!DOCTYPE html>
<html>
...
```

Connection is allowed! Pod can reach internal services.

### Task 4: Test External Network Communication (Blocked)
**Priority:** High
**Status:** pending

Verify that external connections are blocked.

**Actions:**
- Run test pod with label `app=foo` (or use existing pod)
- Attempt connection to external service
- Verify connection is blocked

**Commands:**
```bash
# Inside the same pod:
wget -O- --timeout 1 http://www.example.com
```

**Expected Result:**
```
Connecting to www.example.com (93.184.216.34:80)
wget: download timed out
```

Connection is blocked! The pod can resolve the IP address of `www.example.com`, however it cannot establish a connection. Effectively, external traffic is blocked.

### Task 5: Verify DNS Resolution Works
**Priority:** High
**Status:** pending

Confirm DNS resolution is functioning for both internal and external names.

**Actions:**
- Test DNS resolution for internal service
- Test DNS resolution for external service
- Verify both resolve successfully

**Commands:**
```bash
# Inside the test pod:
nslookup web
nslookup www.example.com
```

**Expected Result:**
Both names resolve to IP addresses successfully. DNS is working, but actual connections to external IPs are blocked.

## Acceptance Criteria

- [ ] NetworkPolicy `foo-deny-external-egress` created successfully
- [ ] Policy targets pods with `app=foo` label
- [ ] Policy specifies Egress policyType
- [ ] DNS access to kube-dns is allowed
- [ ] Both UDP and TCP port 53 are allowed for DNS
- [ ] Internal cluster communication is allowed
- [ ] DNS resolution works for internal and external names
- [ ] Connections to external IP addresses are blocked
- [ ] Web service accessible internally

## Technical Specifications

**NetworkPolicy Configuration:**
- Name: `foo-deny-external-egress`
- Pod Selector: `app=foo`
- Policy Types: Egress
- Egress Rules: DNS only (to kube-dns)

**How It Works:**
- Policy targets pods with `app=foo` label
- Only explicit egress rule is for DNS
- DNS resolution allowed to kube-dns in kube-system
- No rules for external IPs means they're blocked
- Internal pod-to-pod communication is allowed by Kubernetes default
- External traffic is implicitly denied

**Traffic Flow:**

**Allowed:**
```
Pod (app=foo)
    ↓ DNS query
kube-dns (kube-system) ✓
    ↓ DNS response
Pod (app=foo)
    ↓ Connection to internal service IP
Internal Service (web) ✓
```

**Blocked:**
```
Pod (app=foo)
    ↓ DNS query
kube-dns (kube-system) ✓
    ↓ DNS response with external IP
Pod (app=foo)
    ↓ Connection to external IP
External IP (93.184.216.34) ✗ BLOCKED
```

**Why Internal Traffic Works:**
By default, Kubernetes allows all pod-to-pod traffic unless restricted by NetworkPolicy. This policy only restricts egress to external destinations. Internal cluster IPs are not restricted because:
1. No ipBlock rules are specified
2. Kubernetes defaults allow internal communication
3. Only external traffic (IPs outside cluster CIDR) is blocked

## Implementation Details

**Understanding Internal vs External:**

**This Policy (Blocks External Only):**
```yaml
egress:
- to:
  - namespaceSelector: {...}
    podSelector: {...}
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
# No other rules = external IPs blocked
# Internal pod IPs allowed by default
```

**Alternative: Explicit Internal Allow:**
```yaml
egress:
# DNS
- to:
  - namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: kube-system
    podSelector:
      matchLabels:
        k8s-app: kube-dns
  ports:
  - port: 53
    protocol: UDP
  - port: 53
    protocol: TCP
# Internal cluster CIDR
- to:
  - ipBlock:
      cidr: 10.0.0.0/8  # Cluster internal CIDR
# Specific external IPs (if needed)
- to:
  - ipBlock:
      cidr: 203.0.113.0/24
  ports:
  - port: 443
```

**Using ipBlock for Precise Control:**
```yaml
egress:
- to:
  # Allow internal cluster network
  - ipBlock:
      cidr: 10.0.0.0/8
      except:
      - 10.0.1.0/24  # Block specific internal subnet
  # Allow specific external service
  - ipBlock:
      cidr: 203.0.113.10/32
  ports:
  - port: 443
```

**Pattern: Database with Limited External Access:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-internal-only
spec:
  podSelector:
    matchLabels:
      app: database
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Internal cluster only (no external traffic)
```

**Pattern: Backend with Specific External API:**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-limited-external
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Specific external API
  - to:
    - ipBlock:
        cidr: 52.1.2.3/32  # Specific external API server
    ports:
    - port: 443
```

## Verification

Check policy and test connectivity:
```bash
# View NetworkPolicy
kubectl get networkpolicy
kubectl describe networkpolicy foo-deny-external-egress

# Test internal connectivity
kubectl run test-internal --labels="app=foo" --rm -i -t --image=alpine -- sh
# Inside:
wget -O- http://web
nslookup web

# Test external connectivity (should fail)
kubectl run test-external --labels="app=foo" --rm -i -t --image=alpine -- sh
# Inside:
wget -O- --timeout=2 http://www.example.com
ping 8.8.8.8

# Check DNS resolution
kubectl run test-dns --labels="app=foo" --rm -i -t --image=alpine -- sh
# Inside:
nslookup google.com  # Should resolve
wget -O- --timeout=2 http://google.com  # Should timeout
```

## Cleanup

### Task: Remove Resources
Remove all created resources:

```bash
kubectl delete pod,service web
kubectl delete networkpolicy foo-deny-external-egress
```

## References

- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Egress Policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-deny-all-egress-traffic)
- [IP Block](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#ipblock-v1-networking-k8s-io)

## Notes

**Best Practices:**
- Always allow DNS unless you have a specific reason not to
- Document which services need external access and why
- Use ipBlock for explicit external service allowlisting
- Test both internal and external connectivity
- Monitor for blocked legitimate external requests

**Common Use Cases:**

**Backend Service (No External Access):**
```yaml
# Backend that only talks to internal services
spec:
  podSelector:
    matchLabels:
      tier: backend
  policyTypes:
  - Egress
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
```

**Payment Service (Specific External Gateway):**
```yaml
# Payment service to specific payment gateway
spec:
  podSelector:
    matchLabels:
      app: payment-service
  policyTypes:
  - Egress
  egress:
  # DNS
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
      podSelector:
        matchLabels:
          k8s-app: kube-dns
    ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP
  # Payment gateway
  - to:
    - ipBlock:
        cidr: 52.123.45.67/32
    ports:
    - port: 443
```

**Meme Reference:**
The meme below can be used to explain how your cluster looks like with a policy like this:

![Network Policy Meme](https://user-images.githubusercontent.com/14810215/126358758-5b14dcd3-79df-4f85-a248-ee6b36e2e90e.png)

*Source: https://twitter.com/memenetes/status/1417227948206211082*

**Debugging Tips:**
```bash
# Check cluster CIDR range
kubectl cluster-info dump | grep -i cidr

# Test DNS resolution
kubectl exec -it <pod> -- nslookup kubernetes.default

# Test internal connectivity
kubectl exec -it <pod> -- nc -zv <service-name> <port>

# Test external connectivity (should fail)
kubectl exec -it <pod> -- nc -zv 8.8.8.8 53

# View effective NetworkPolicies
kubectl get networkpolicy -A
kubectl describe networkpolicy <name>

# Check kube-dns endpoints
kubectl get endpoints -n kube-system kube-dns
```

**Common Mistakes:**
- Not allowing DNS (leads to service discovery failures)
- Blocking legitimate external dependencies (APIs, databases)
- Not understanding cluster CIDR ranges
- Assuming internal traffic is automatically blocked
- Forgetting about IPv6 addresses

**Security Considerations:**
- This is not a replacement for firewall rules
- Combine with other security measures:
  - Pod Security Standards
  - RBAC
  - Service mesh policies
  - Cloud provider security groups
- Regularly audit external access requirements
- Monitor for policy violations

**GKE Specific Notes:**
- Requires GKE version 1.8.4-gke.0 or later
- GKE cluster CIDR is typically 10.0.0.0/8 or 10.4.0.0/14
- Check with: `gcloud container clusters describe <cluster> --format="value(clusterIpv4Cidr)"`
- Consider GKE Dataplane V2 for better performance

**Determining Cluster CIDR:**
```bash
# Method 1: Check cluster info
kubectl cluster-info dump | grep -i "cluster-cidr"

# Method 2: Check node pod CIDR
kubectl get nodes -o jsonpath='{.items[*].spec.podCIDR}'

# Method 3: Check service CIDR
kubectl cluster-info dump | grep -i "service-cluster-ip-range"

# Method 4: GKE specific
gcloud container clusters describe <cluster-name> --format="value(clusterIpv4Cidr,servicesIpv4Cidr)"

# Method 5: EKS specific
aws eks describe-cluster --name <cluster-name> --query cluster.kubernetesNetworkConfig
```

**Advanced Patterns:**

**Multi-Region Cluster:**
```yaml
egress:
# Allow internal cluster (all regions)
- to:
  - ipBlock:
      cidr: 10.0.0.0/8
  - ipBlock:
      cidr: 172.16.0.0/12
  - ipBlock:
      cidr: 192.168.0.0/16
```

**Hybrid Cloud:**
```yaml
egress:
# Internal cluster
- to:
  - ipBlock:
      cidr: 10.0.0.0/8
# On-premises datacenter
- to:
  - ipBlock:
      cidr: 172.20.0.0/16
  ports:
  - port: 1521  # Oracle DB
  - port: 3306  # MySQL
```

**Service Mesh Integration:**
When using a service mesh (Istio, Linkerd), NetworkPolicies work alongside service mesh policies:
- NetworkPolicy operates at L3/L4 (IP/port level)
- Service mesh operates at L7 (application level)
- Both are enforced - most restrictive wins
- Use NetworkPolicy for broad restrictions
- Use service mesh for fine-grained L7 policies

**Performance Considerations:**
- Egress policies can impact network performance
- Test latency after applying policies
- Monitor for increased connection setup time
- Consider policy complexity vs. performance trade-offs

**Monitoring:**
```bash
# Monitor denied connections (CNI dependent)
# Calico example:
kubectl logs -n calico-system -l k8s-app=calico-node | grep denied

# Cilium example:
cilium monitor --type drop --related-to <pod-name>

# Check NetworkPolicy status
kubectl get networkpolicy -o yaml

# View logs from affected pods
kubectl logs <pod-name> | grep -i "connection\|timeout\|refused"
```

This pattern is essential for implementing defense-in-depth security by preventing unauthorized external communication while maintaining necessary internal cluster connectivity.
# Amp Instructions

## Task Master AI Instructions
**Import Task Master's development workflow commands and guidelines, treat as if import is in the main AGENT.md file.**
@./.taskmaster/AGENT.md
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._
# Claude Code Instructions

## Task Master AI Instructions
**Import Task Master's development workflow commands and guidelines, treat as if import is in the main CLAUDE.md file.**
@./.taskmaster/CLAUDE.md
---
id: NP-CONTRIBUTING
title: Contributing to Kubernetes Network Policy Recipes
type: documentation
category: meta
priority: medium
status: ready
estimated_time: 10m
dependencies: []
tags: [contributing, community, cla, code-review]
---

## Overview

We welcome your contributions to this Kubernetes Network Policy Recipes project! This guide outlines the process for contributing patches, improvements, and new recipes to the repository.

## Objectives

- Understand the contribution process
- Complete necessary legal requirements
- Follow code review procedures
- Maintain quality and consistency
- Build a collaborative community

## Contribution Requirements

### Contributor License Agreement

Contributions to this project must be accompanied by a Contributor License Agreement (CLA). You (or your employer) retain the copyright to your contribution; this simply gives us permission to use and redistribute your contributions as part of the project.

**CLA Submission:**
- Visit <https://cla.developers.google.com/> to see your current agreements on file or to sign a new one
- You generally only need to submit a CLA once
- If you've already submitted one (even if it was for a different project), you probably don't need to do it again

**Important Notes:**
- Individual contributors sign individual CLA
- Corporate contributors require corporate CLA signed by employer
- CLA must be on file before pull requests can be merged

## Code Review Process

### Pull Request Requirements

All submissions, including submissions by project members, require review. We use GitHub pull requests for this purpose.

**Before Submitting:**
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test your changes thoroughly
5. Commit with clear, descriptive messages
6. Push to your fork
7. Open a pull request

**Pull Request Guidelines:**
- Clear title describing the change
- Detailed description of what changed and why
- Reference any related issues
- Include testing steps if applicable
- Ensure all tests pass
- Follow existing code style and formatting

### Review Process

**Timeline:**
- Initial review typically within 2-3 business days
- Follow-up reviews within 1-2 business days
- Merge after approval from maintainers

**Review Criteria:**
- Code quality and correctness
- Documentation completeness
- Adherence to project standards
- Test coverage (if applicable)
- Security considerations
- Performance implications

**Addressing Feedback:**
- Respond to all review comments
- Make requested changes promptly
- Push updates to the same branch
- Request re-review when ready

## Contribution Types

### New NetworkPolicy Recipes

When contributing new recipes:

**Required Elements:**
- YAML frontmatter with metadata (id, title, type, category, priority, status, estimated_time, dependencies, tags)
- Clear overview and objectives
- Comprehensive background and use cases
- Step-by-step requirements with tasks
- Complete YAML manifests
- Testing instructions
- Acceptance criteria
- Technical specifications
- Implementation details
- Verification steps
- Cleanup procedures
- References and notes

**Recipe Template:**
```yaml
---
id: NP-XX
title: Recipe Title
type: policy
category: [basics|namespaces|advanced|egress]
priority: [high|medium|low]
status: ready
estimated_time: XXm
dependencies: [NP-00]
tags: [relevant, tags, here]
---

## Overview
## Objectives
## Background
## Requirements
## Acceptance Criteria
## Technical Specifications
## Implementation Details
## Verification
## Cleanup
## References
## Notes
```

### Documentation Improvements

We welcome:
- Clarifications and corrections
- Additional examples
- Troubleshooting tips
- Best practices
- Common patterns
- Performance notes
- Security considerations

### Bug Reports

When reporting bugs:
- Use GitHub Issues
- Provide clear description
- Include steps to reproduce
- Share relevant environment details
- Attach logs if applicable
- Specify expected vs actual behavior

### Feature Requests

For new features:
- Check existing issues first
- Describe the use case
- Explain expected benefits
- Suggest implementation approach
- Consider backward compatibility

## Style Guidelines

### Markdown Formatting

- Use ATX-style headers (# Header)
- Wrap lines at 80-100 characters where practical
- Use code blocks with language specification
- Include alt text for images
- Use consistent bullet point style

### YAML Style

- 2-space indentation
- No tabs
- Quote strings when necessary
- Comment complex configurations
- Follow Kubernetes conventions

### Command Examples

```bash
# Use clear, copy-pasteable commands
kubectl apply -f policy.yaml

# Show expected output
# Expected:
# networkpolicy "example" created

# Include comments for clarity
```

## Testing

### Manual Testing

Before submitting:
1. Create test cluster
2. Apply the policy
3. Verify expected behavior
4. Test edge cases
5. Clean up resources
6. Document test results

### Test Checklist

- [ ] Policy applies without errors
- [ ] Target pods are correctly selected
- [ ] Allowed traffic flows as expected
- [ ] Blocked traffic is actually blocked
- [ ] DNS resolution works (if applicable)
- [ ] Cleanup removes all resources
- [ ] Instructions are clear and accurate

## Community Guidelines

### Code of Conduct

- Be respectful and inclusive
- Welcome newcomers
- Provide constructive feedback
- Focus on the technical merits
- Assume good intentions
- Help others learn

### Communication

- GitHub Issues for bugs and features
- Pull requests for contributions
- Discussions for questions
- Kubernetes Slack #sig-network for community chat

## Getting Help

### Resources

- [GitHub Help - Pull Requests](https://help.github.com/articles/about-pull-requests/)
- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [Kubernetes Contributor Guide](https://kubernetes.io/docs/contribute/)

### Questions

- Check existing issues and documentation first
- Search closed issues for previous discussions
- Ask in GitHub Discussions
- Join Kubernetes Slack

## Recognition

Contributors are recognized in:
- Git commit history
- GitHub contributors page
- Release notes (for significant contributions)
- Project acknowledgments

## License

By contributing, you agree that your contributions will be licensed under the Apache License 2.0, the same license as the project.

## Acknowledgments

Thank you for contributing to making Kubernetes networking more secure and accessible for everyone!

---

For questions about this process, please open an issue or reach out to the maintainers.
# Task Master AI - Agent Integration Guide

## Essential Commands

### Core Workflow Commands

```bash
# Project Setup
task-master init                                    # Initialize Task Master in current project
task-master parse-prd .taskmaster/docs/prd.txt      # Generate tasks from PRD document
task-master models --setup                        # Configure AI models interactively

# Daily Development Workflow
task-master list                                   # Show all tasks with status
task-master next                                   # Get next available task to work on
task-master show <id>                             # View detailed task information (e.g., task-master show 1.2)
task-master set-status --id=<id> --status=done    # Mark task complete

# Task Management
task-master add-task --prompt="description" --research        # Add new task with AI assistance
task-master expand --id=<id> --research --force              # Break task into subtasks
task-master update-task --id=<id> --prompt="changes"         # Update specific task
task-master update --from=<id> --prompt="changes"            # Update multiple tasks from ID onwards
task-master update-subtask --id=<id> --prompt="notes"        # Add implementation notes to subtask

# Analysis & Planning
task-master analyze-complexity --research          # Analyze task complexity
task-master complexity-report                      # View complexity analysis
task-master expand --all --research               # Expand all eligible tasks

# Dependencies & Organization
task-master add-dependency --id=<id> --depends-on=<id>       # Add task dependency
task-master move --from=<id> --to=<id>                       # Reorganize task hierarchy
task-master validate-dependencies                            # Check for dependency issues
task-master generate                                         # Update task markdown files (usually auto-called)
```

## Key Files & Project Structure

### Core Files

- `.taskmaster/tasks/tasks.json` - Main task data file (auto-managed)
- `.taskmaster/config.json` - AI model configuration (use `task-master models` to modify)
- `.taskmaster/docs/prd.txt` - Product Requirements Document for parsing
- `.taskmaster/tasks/*.txt` - Individual task files (auto-generated from tasks.json)
- `.env` - API keys for CLI usage

### Claude Code Integration Files

- `CLAUDE.md` - Auto-loaded context for Claude Code (this file)
- `.claude/settings.json` - Claude Code tool allowlist and preferences
- `.claude/commands/` - Custom slash commands for repeated workflows
- `.mcp.json` - MCP server configuration (project-specific)

### Directory Structure

```
project/
├── .taskmaster/
│   ├── tasks/              # Task files directory
│   │   ├── tasks.json      # Main task database
│   │   ├── task-1.md      # Individual task files
│   │   └── task-2.md
│   ├── docs/              # Documentation directory
│   │   ├── prd.txt        # Product requirements
│   ├── reports/           # Analysis reports directory
│   │   └── task-complexity-report.json
│   ├── templates/         # Template files
│   │   └── example_prd.txt  # Example PRD template
│   └── config.json        # AI models & settings
├── .claude/
│   ├── settings.json      # Claude Code configuration
│   └── commands/         # Custom slash commands
├── .env                  # API keys
├── .mcp.json            # MCP configuration
└── CLAUDE.md            # This file - auto-loaded by Claude Code
```

## MCP Integration

Task Master provides an MCP server that Claude Code can connect to. Configure in `.mcp.json`:

```json
{
  "mcpServers": {
    "task-master-ai": {
      "command": "npx",
      "args": ["-y", "task-master-ai"],
      "env": {
        "ANTHROPIC_API_KEY": "your_key_here",
        "PERPLEXITY_API_KEY": "your_key_here",
        "OPENAI_API_KEY": "OPENAI_API_KEY_HERE",
        "GOOGLE_API_KEY": "GOOGLE_API_KEY_HERE",
        "XAI_API_KEY": "XAI_API_KEY_HERE",
        "OPENROUTER_API_KEY": "OPENROUTER_API_KEY_HERE",
        "MISTRAL_API_KEY": "MISTRAL_API_KEY_HERE",
        "AZURE_OPENAI_API_KEY": "AZURE_OPENAI_API_KEY_HERE",
        "OLLAMA_API_KEY": "OLLAMA_API_KEY_HERE"
      }
    }
  }
}
```

### Essential MCP Tools

```javascript
help; // = shows available taskmaster commands
// Project setup
initialize_project; // = task-master init
parse_prd; // = task-master parse-prd

// Daily workflow
get_tasks; // = task-master list
next_task; // = task-master next
get_task; // = task-master show <id>
set_task_status; // = task-master set-status

// Task management
add_task; // = task-master add-task
expand_task; // = task-master expand
update_task; // = task-master update-task
update_subtask; // = task-master update-subtask
update; // = task-master update

// Analysis
analyze_project_complexity; // = task-master analyze-complexity
complexity_report; // = task-master complexity-report
```

## Claude Code Workflow Integration

### Standard Development Workflow

#### 1. Project Initialization

```bash
# Initialize Task Master
task-master init

# Create or obtain PRD, then parse it
task-master parse-prd .taskmaster/docs/prd.txt

# Analyze complexity and expand tasks
task-master analyze-complexity --research
task-master expand --all --research
```

If tasks already exist, another PRD can be parsed (with new information only!) using parse-prd with --append flag. This will add the generated tasks to the existing list of tasks..

#### 2. Daily Development Loop

```bash
# Start each session
task-master next                           # Find next available task
task-master show <id>                     # Review task details

# During implementation, check in code context into the tasks and subtasks
task-master update-subtask --id=<id> --prompt="implementation notes..."

# Complete tasks
task-master set-status --id=<id> --status=done
```

#### 3. Multi-Claude Workflows

For complex projects, use multiple Claude Code sessions:

```bash
# Terminal 1: Main implementation
cd project && claude

# Terminal 2: Testing and validation
cd project-test-worktree && claude

# Terminal 3: Documentation updates
cd project-docs-worktree && claude
```

### Custom Slash Commands

Create `.claude/commands/taskmaster-next.md`:

```markdown
Find the next available Task Master task and show its details.

Steps:

1. Run `task-master next` to get the next task
2. If a task is available, run `task-master show <id>` for full details
3. Provide a summary of what needs to be implemented
4. Suggest the first implementation step
```

Create `.claude/commands/taskmaster-complete.md`:

```markdown
Complete a Task Master task: $ARGUMENTS

Steps:

1. Review the current task with `task-master show $ARGUMENTS`
2. Verify all implementation is complete
3. Run any tests related to this task
4. Mark as complete: `task-master set-status --id=$ARGUMENTS --status=done`
5. Show the next available task with `task-master next`
```

## Tool Allowlist Recommendations

Add to `.claude/settings.json`:

```json
{
  "allowedTools": [
    "Edit",
    "Bash(task-master *)",
    "Bash(git commit:*)",
    "Bash(git add:*)",
    "Bash(npm run *)",
    "mcp__task_master_ai__*"
  ]
}
```

## Configuration & Setup

### API Keys Required

At least **one** of these API keys must be configured:

- `ANTHROPIC_API_KEY` (Claude models) - **Recommended**
- `PERPLEXITY_API_KEY` (Research features) - **Highly recommended**
- `OPENAI_API_KEY` (GPT models)
- `GOOGLE_API_KEY` (Gemini models)
- `MISTRAL_API_KEY` (Mistral models)
- `OPENROUTER_API_KEY` (Multiple models)
- `XAI_API_KEY` (Grok models)

An API key is required for any provider used across any of the 3 roles defined in the `models` command.

### Model Configuration

```bash
# Interactive setup (recommended)
task-master models --setup

# Set specific models
task-master models --set-main claude-3-5-sonnet-20241022
task-master models --set-research perplexity-llama-3.1-sonar-large-128k-online
task-master models --set-fallback gpt-4o-mini
```

## Task Structure & IDs

### Task ID Format

- Main tasks: `1`, `2`, `3`, etc.
- Subtasks: `1.1`, `1.2`, `2.1`, etc.
- Sub-subtasks: `1.1.1`, `1.1.2`, etc.

### Task Status Values

- `pending` - Ready to work on
- `in-progress` - Currently being worked on
- `done` - Completed and verified
- `deferred` - Postponed
- `cancelled` - No longer needed
- `blocked` - Waiting on external factors

### Task Fields

```json
{
  "id": "1.2",
  "title": "Implement user authentication",
  "description": "Set up JWT-based auth system",
  "status": "pending",
  "priority": "high",
  "dependencies": ["1.1"],
  "details": "Use bcrypt for hashing, JWT for tokens...",
  "testStrategy": "Unit tests for auth functions, integration tests for login flow",
  "subtasks": []
}
```

## Claude Code Best Practices with Task Master

### Context Management

- Use `/clear` between different tasks to maintain focus
- This CLAUDE.md file is automatically loaded for context
- Use `task-master show <id>` to pull specific task context when needed

### Iterative Implementation

1. `task-master show <subtask-id>` - Understand requirements
2. Explore codebase and plan implementation
3. `task-master update-subtask --id=<id> --prompt="detailed plan"` - Log plan
4. `task-master set-status --id=<id> --status=in-progress` - Start work
5. Implement code following logged plan
6. `task-master update-subtask --id=<id> --prompt="what worked/didn't work"` - Log progress
7. `task-master set-status --id=<id> --status=done` - Complete task

### Complex Workflows with Checklists

For large migrations or multi-step processes:

1. Create a markdown PRD file describing the new changes: `touch task-migration-checklist.md` (prds can be .txt or .md)
2. Use Taskmaster to parse the new prd with `task-master parse-prd --append` (also available in MCP)
3. Use Taskmaster to expand the newly generated tasks into subtasks. Consdier using `analyze-complexity` with the correct --to and --from IDs (the new ids) to identify the ideal subtask amounts for each task. Then expand them.
4. Work through items systematically, checking them off as completed
5. Use `task-master update-subtask` to log progress on each task/subtask and/or updating/researching them before/during implementation if getting stuck

### Git Integration

Task Master works well with `gh` CLI:

```bash
# Create PR for completed task
gh pr create --title "Complete task 1.2: User authentication" --body "Implements JWT auth system as specified in task 1.2"

# Reference task in commits
git commit -m "feat: implement JWT auth (task 1.2)"
```

### Parallel Development with Git Worktrees

```bash
# Create worktrees for parallel task development
git worktree add ../project-auth feature/auth-system
git worktree add ../project-api feature/api-refactor

# Run Claude Code in each worktree
cd ../project-auth && claude    # Terminal 1: Auth work
cd ../project-api && claude     # Terminal 2: API work
```

## Troubleshooting

### AI Commands Failing

```bash
# Check API keys are configured
cat .env                           # For CLI usage

# Verify model configuration
task-master models

# Test with different model
task-master models --set-fallback gpt-4o-mini
```

### MCP Connection Issues

- Check `.mcp.json` configuration
- Verify Node.js installation
- Use `--mcp-debug` flag when starting Claude Code
- Use CLI as fallback if MCP unavailable

### Task File Sync Issues

```bash
# Regenerate task files from tasks.json
task-master generate

# Fix dependency issues
task-master fix-dependencies
```

DO NOT RE-INITIALIZE. That will not do anything beyond re-adding the same Taskmaster core files.

## Important Notes

### AI-Powered Operations

These commands make AI calls and may take up to a minute:

- `parse_prd` / `task-master parse-prd`
- `analyze_project_complexity` / `task-master analyze-complexity`
- `expand_task` / `task-master expand`
- `expand_all` / `task-master expand --all`
- `add_task` / `task-master add-task`
- `update` / `task-master update`
- `update_task` / `task-master update-task`
- `update_subtask` / `task-master update-subtask`

### File Management

- Never manually edit `tasks.json` - use commands instead
- Never manually edit `.taskmaster/config.json` - use `task-master models`
- Task markdown files in `tasks/` are auto-generated
- Run `task-master generate` after manual changes to tasks.json

### Claude Code Session Management

- Use `/clear` frequently to maintain focused context
- Create custom slash commands for repeated Task Master workflows
- Configure tool allowlist to streamline permissions
- Use headless mode for automation: `claude -p "task-master next"`

### Multi-Task Updates

- Use `update --from=<id>` to update multiple future tasks
- Use `update-task --id=<id>` for single task updates
- Use `update-subtask --id=<id>` for implementation logging

### Research Mode

- Add `--research` flag for research-based AI enhancement
- Requires a research model API key like Perplexity (`PERPLEXITY_API_KEY`) in environment
- Provides more informed task creation and updates
- Recommended for complex technical tasks

---

_This guide ensures Claude Code has immediate access to Task Master's essential functionality for agentic development workflows._
---
id: NP-README
title: Kubernetes Network Policy Recipes
type: documentation
category: overview
priority: high
status: ready
estimated_time: 30m
dependencies: []
tags: [network-policy, kubernetes, security, documentation, recipes]
---

## Overview

This repository contains various use cases of Kubernetes Network Policies and sample YAML files to leverage in your setup. If you ever wondered how to drop/restrict traffic to applications running on Kubernetes, this comprehensive guide provides practical recipes and patterns.

![Network Policy in Action](img/1.gif)
*You can get stuff like this with Network Policies...*

## Objectives

- Understand Kubernetes NetworkPolicy concepts and patterns
- Learn how to implement network segmentation in Kubernetes
- Apply zero-trust networking principles
- Secure cluster networking with practical examples
- Master ingress and egress traffic control

## Background

Easiest way to try out Network Policies is to create a new [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine) cluster. Applying Network Policies on your existing cluster can disrupt the networking. At the time of writing, most cloud providers do not provide built-in network policy support.

If you are not familiar with Network Policies at all, I recommend reading the article [Securing Kubernetes Cluster Networking](https://ahmet.im/blog/kubernetes-network-policy/) first.

## NetworkPolicy Crash Course

NetworkPolicies operate at layer 3 or 4 of OSI model (IP and port level). They are used to control the traffic in (ingress) and out (egress) of pods.

### NetworkPolicy Gotchas

- **Empty selector matches everything**: `spec.podSelector: {}` will apply the policy to all pods in the current namespace.

- **Selectors are namespace-scoped**: `spec.podSelector` of an ingress rule can only select pods in the same namespace as the NetworkPolicy.

- **Default allow all**: If no NetworkPolicies target a pod, all traffic to and from the pod is allowed. In other words, all traffic is allowed until a policy is applied.

- **No deny rules**: There are no explicit deny rules in NetworkPolicies. NetworkPolicies are deny-by-default, allow-explicitly. It's the same as saying "If you're not on the list you can't get in."

- **Empty rules block all**: If a NetworkPolicy matches a pod but has an empty/null rule, all traffic is blocked. Example:
  ```yaml
  spec:
    podSelector:
      matchLabels:
        app: web
    ingress: []  # Blocks all ingress
  ```

- **Policies are additive**: NetworkPolicies are additive. If multiple NetworkPolicies select a pod, their union is evaluated and applied to that pod. If ANY policy allows traffic, it flows.

## Repository Structure

### Before You Begin

I really recommend [watching my KubeCon talk on Network Policies](https://www.youtube.com/watch?v=3gGpMmYeEO8) if you want to get a good understanding of this feature. It will help you understand this repository better.

- [Create a cluster](00-create-cluster.md) - **NP-00**

### Basics

Core NetworkPolicy patterns for pod-level traffic control:

- [DENY all traffic to an application](01-deny-all-traffic-to-an-application.md) - **NP-01**
- [LIMIT traffic to an application](02-limit-traffic-to-an-application.md) - **NP-02**
- [ALLOW all traffic to an application](02a-allow-all-traffic-to-an-application.md) - **NP-02A**

### Namespaces

Namespace-level network segmentation patterns:

- [DENY all non-whitelisted traffic in the current namespace](03-deny-all-non-whitelisted-traffic-in-the-namespace.md) - **NP-03**
- [DENY all traffic from other namespaces](04-deny-traffic-from-other-namespaces.md) (a.k.a LIMIT access to the current namespace) - **NP-04**
- [ALLOW traffic to an application from all namespaces](05-allow-traffic-from-all-namespaces.md) - **NP-05**
- [ALLOW all traffic from a namespace](06-allow-traffic-from-a-namespace.md) - **NP-06**
- [ALLOW traffic from some pods in another namespace](07-allow-traffic-from-some-pods-in-another-namespace.md) - **NP-07**

### Serving External Traffic

Patterns for exposing services to external clients:

- [ALLOW traffic from external clients](08-allow-external-traffic.md) - **NP-08**

### Advanced

Advanced patterns for fine-grained control:

- [ALLOW traffic only to certain port numbers of an application](09-allow-traffic-only-to-a-port.md) - **NP-09**
- [ALLOW traffic from apps using multiple selectors](10-allowing-traffic-with-multiple-selectors.md) - **NP-10**

### Controlling Outbound (Egress) Traffic

Egress traffic control patterns:

- [DENY egress traffic from an application](11-deny-egress-traffic-from-an-application.md) - **NP-11**
- [DENY all non-whitelisted egress traffic in a namespace](12-deny-all-non-whitelisted-traffic-from-the-namespace.md) - **NP-12**
- [LIMIT egress traffic to the cluster (DENY external egress traffic)](14-deny-external-egress-traffic.md) - **NP-14**

**Coming Soon:**
- LIMIT egress traffic from an application to some pods
- ALLOW traffic only to Pods in a namespace

## Quick Reference

### Policy Types by Use Case

**Security Posture:**
- Default Deny All: NP-01, NP-03, NP-11, NP-12
- Whitelisting: NP-02, NP-06, NP-07, NP-10
- External Access: NP-08

**Namespace Isolation:**
- Same Namespace Only: NP-04
- Cross-Namespace: NP-05, NP-06, NP-07

**Advanced Control:**
- Port-Level: NP-09
- Multiple Selectors: NP-10
- Egress Control: NP-11, NP-12, NP-14

### Common Patterns

**Zero-Trust Foundation:**
1. Apply default-deny ingress (NP-03)
2. Apply default-deny egress (NP-12)
3. Whitelist necessary traffic explicitly

**Microservices Security:**
1. Limit traffic between services (NP-02)
2. Use multiple selectors for shared resources (NP-10)
3. Port-based segmentation (NP-09)

**Multi-Tenancy:**
1. Namespace isolation (NP-04)
2. Selective cross-namespace access (NP-06, NP-07)
3. External traffic control (NP-08)

## Prerequisites

- Kubernetes cluster v1.7+ with NetworkPolicy support
- Network plugin that implements NetworkPolicy (Calico, Cilium, Weave Net, etc.)
- kubectl configured to access your cluster
- Basic understanding of Kubernetes concepts

## Testing Your Setup

Verify NetworkPolicy support:
```bash
kubectl api-versions | grep networking.k8s.io/v1
```

Check your network plugin:
```bash
kubectl get pods -n kube-system | grep -E 'calico|cilium|weave'
```

## Best Practices

1. **Start with Deny-All**: Begin with default-deny policies, then whitelist necessary traffic
2. **Label Consistently**: Use consistent labeling schemes across your applications
3. **Document Dependencies**: Maintain documentation of service dependencies
4. **Test Thoroughly**: Test policies in non-production before applying to production
5. **Monitor Traffic**: Use network flow logs to understand traffic patterns
6. **Progressive Rollout**: Apply policies gradually, starting with dev/staging
7. **Combine with RBAC**: Use NetworkPolicies alongside RBAC for defense-in-depth
8. **Regular Audits**: Periodically review and update policies

## Troubleshooting

### Policy Not Working
```bash
# Check if policy exists
kubectl get networkpolicy

# Describe policy details
kubectl describe networkpolicy <policy-name>

# Verify pod labels match
kubectl get pods --show-labels

# Check CNI plugin logs
kubectl logs -n kube-system -l k8s-app=<cni-plugin>
```

### Connection Issues
```bash
# Test connectivity from pod
kubectl exec -it <pod> -- wget -qO- --timeout=2 http://<service>

# Check DNS resolution
kubectl exec -it <pod> -- nslookup <service>

# Verify service endpoints
kubectl get endpoints <service>
```

### Common Issues
- CNI plugin doesn't support NetworkPolicy
- Labels don't match selectors
- Namespace not specified correctly
- DNS traffic not whitelisted
- Policies applied to wrong namespace

## Performance Considerations

- NetworkPolicies are evaluated at the CNI plugin level
- Large numbers of policies can impact performance
- Test scalability with your specific CNI plugin
- Monitor for increased latency after applying policies
- Consider policy complexity vs. performance trade-offs

## Security Considerations

NetworkPolicies are **defense-in-depth**, not a silver bullet:

- Combine with Pod Security Standards/Policies
- Use alongside RBAC for comprehensive access control
- Implement application-level authentication
- Use service mesh for L7 policies
- Monitor and log policy violations
- Regular security audits

## Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## Resources

### Official Documentation
- [Kubernetes NetworkPolicy Documentation](https://kubernetes.io/docs/concepts/services-networking/network-policies/)
- [NetworkPolicy API Reference](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#networkpolicy-v1-networking-k8s-io)

### Videos & Talks
- [KubeCon Talk: Securing Kubernetes Network](https://www.youtube.com/watch?v=3gGpMmYeEO8)
- [Securing Kubernetes Cluster Networking](https://ahmet.im/blog/kubernetes-network-policy/)

### Tools
- [Calico Network Policy Editor](https://editor.cilium.io/)
- [Network Policy Viewer](https://github.com/runoncloud/network-policy-viewer)

### CNI Plugins with NetworkPolicy Support
- [Calico](https://www.tigera.io/project-calico/)
- [Cilium](https://cilium.io/)
- [Weave Net](https://www.weave.works/oss/net/)
- [Antrea](https://antrea.io/)

## Community & Support

- **Issues**: Report issues or request features via [GitHub Issues](https://github.com/ahmetb/kubernetes-networkpolicy-tutorial/issues)
- **Discussions**: Join discussions on Kubernetes Slack #sig-network
- **Updates**: Star/watch this repository for updates

## License

Copyright 2017, Google Inc. Distributed under Apache License Version 2.0.
See [LICENSE](LICENSE) for details.

**Disclaimer:** This is not an official Google product.

## Acknowledgments

Created by Ahmet Alp Balkan ([@ahmetb](https://twitter.com/ahmetb)).

Special thanks to all contributors and the Kubernetes networking community.

---

![Stargazers over time](https://starcharts.herokuapp.com/ahmetb/kubernetes-networkpolicy-tutorial.svg)

## Quick Start Guide

### 1. Create Cluster
```bash
# GKE with Network Policy support
gcloud container clusters create np \
    --enable-network-policy \
    --zone us-central1-b
```

### 2. Apply Default-Deny
```bash
# Deny all ingress to namespace
kubectl apply -f 03-deny-all-non-whitelisted-traffic-in-the-namespace.md

# Deny all egress from namespace
kubectl apply -f 12-deny-all-non-whitelisted-traffic-from-the-namespace.md
```

### 3. Whitelist Required Traffic
```bash
# Allow specific services to communicate
kubectl apply -f 02-limit-traffic-to-an-application.md
```

### 4. Verify
```bash
# Check policies
kubectl get networkpolicy

# Test connectivity
kubectl run test --rm -i -t --image=alpine -- sh
```

Start with the recipes that match your use case and build from there!
